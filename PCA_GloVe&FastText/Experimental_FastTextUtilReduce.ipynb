{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Zh9T9zQtYosT",
        "We5NFxheYzoF",
        "rsmNXAQTYto6",
        "ehvoqE1XcNSx",
        "Wyt3qxWIY7LT",
        "zTDgEVeTmTEt",
        "kLFh57qscStc",
        "IOswuLauiQcq"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPmynENjD415mVkMieZT6HF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenPeutz/Masterthesis-Disinformation-NLP/blob/master/PCA_GloVe%26FastText/Experimental_FastTextUtilReduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension Reduction FastText (Alternative methods & Experimentation)"
      ],
      "metadata": {
        "id": "zEH0gDJYvKgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fasttext (reduced to 50 dim)\n"
      ],
      "metadata": {
        "id": "fzX-d5qEJhHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install fasttext #==0.6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvtIuFhWJs0q",
        "outputId": "bbcceeeb-f058-44f8-bc6a-2a2a5f88572b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4396453 sha256=01d3b3435b360d2f29fdf5f426aa5731072cf5eaecee52957113c1c0383d9267\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "Zh9T9zQtYosT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APe8mDu1Jble"
      },
      "outputs": [],
      "source": [
        "import fasttext\n",
        "#import fasttext.util"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "fEQTxUQfNvsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "MTfGoJyJNTtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen"
      ],
      "metadata": {
        "id": "RsVb8BxoaVdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "XxtjtnO8KuNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting"
      ],
      "metadata": {
        "id": "We5NFxheYzoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SriTkmUgY1wr",
        "outputId": "d2801bf8-5320-4107-d768-31ceb5988372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepping columns etc"
      ],
      "metadata": {
        "id": "rsmNXAQTYto6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathTESTcsv = '/content/drive/MyDrive/MYDATA/full_TEST01_NX.csv' #be mindful, this is Test-INC (as the INC is the one with the translation columns)\n",
        "pathTRAINcsv = '/content/drive/MyDrive/MYDATA/df_full2_TRAIN01split.csv' #NTS: this trainset is not capped at 2800 chars.. (the testset is..)\n",
        "\n",
        "df_test = pd.read_csv(pathTESTcsv)\n",
        "df_train = pd.read_csv(pathTRAINcsv)"
      ],
      "metadata": {
        "id": "2vU9HRzDNdvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.sample(1000)"
      ],
      "metadata": {
        "id": "gJUrOG-hN3qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['text']\n",
        "y_train = df_train['label']\n",
        "X_testN0 = df_test['text']\n",
        "X_testN1 = df_test['text_N1']\n",
        "X_testN2 = df_test['text_N2']\n",
        "X_testN3 = df_test['text_N3']\n",
        "y_testALL = df_test['label']"
      ],
      "metadata": {
        "id": "NNECUx6ROIKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tokenizing, sequencing and padding..\n"
      ],
      "metadata": {
        "id": "ehvoqE1XcNSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer = Tokenizer(num_words = 1000)\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = word_tokenizer.word_index\n",
        "vocab_length = len(word_index) + 1 #cant be unique words recheck this. Probably need cleaning of data first so that e.g. @CNN is not counted as unique word\n",
        "# https://github.com/keras-team/keras/issues/7551\n",
        "#vocab_length -> 267981 / 231677"
      ],
      "metadata": {
        "id": "BVQp5eODMX24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert series to list (prior to this 2.7GB used)\n",
        "X_train_sequences = word_tokenizer.texts_to_sequences(X_train)\n",
        "# ---\n",
        "X_testN0_sequences = word_tokenizer.texts_to_sequences(X_testN0)\n",
        "X_testN1_sequences = word_tokenizer.texts_to_sequences(X_testN1)\n",
        "X_testN2_sequences = word_tokenizer.texts_to_sequences(X_testN2)\n",
        "X_testN3_sequences = word_tokenizer.texts_to_sequences(X_testN3)"
      ],
      "metadata": {
        "id": "-PmbEfKXMcsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the results to inspect what is happening\n",
        "print(\"Word index for example word 'home' :\\n\", word_index.get('home')) \n",
        "print(\"\\nTraining sequence of first doc:\\n\", X_train_sequences[0]) #first doc, '106' is index of first word of this doc\n",
        "print(\"\\nX_train_sequences data type:\", type(X_train_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaPWBUjWMfT5",
        "outputId": "8629ef9e-bf01-4c4a-91d9-4cd6bc462603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index for example word 'home' :\n",
            " 286\n",
            "\n",
            "Training sequence of first doc:\n",
            " [474, 4, 49, 442, 271, 10, 8, 258, 2, 51, 8, 967, 53, 5, 833, 51, 6, 4, 2, 1, 968, 3, 865, 721, 36, 11, 97, 22, 249, 146, 834, 69, 866, 5, 51, 411, 69, 596, 2, 866, 1, 5, 644, 12, 521, 2, 16, 721, 30, 24, 691, 505, 77, 197, 45, 16, 1, 97, 15, 1, 188, 442, 188, 531, 442, 38, 21, 753, 69, 1, 506, 114, 3, 11, 708, 6, 195, 721, 6, 30, 737, 432, 738, 24, 1, 153, 2, 200, 2, 497, 6, 40, 286, 777, 33, 25, 22, 355, 2, 967, 189, 18, 155, 5, 111, 5, 1, 3, 31, 362, 139, 9, 7, 362, 38, 79, 21, 5, 1, 109, 38, 21, 941, 2, 721, 8, 1, 812, 3, 4, 362, 7, 34, 11, 813, 334, 1, 1, 271, 103, 86, 196, 1, 223, 969, 1, 402, 867, 43, 22, 15, 12, 38, 970, 355, 87, 97, 8, 531, 5, 721, 2, 134, 40, 87, 1, 866, 335, 3, 1, 269, 223, 98, 22, 64, 721, 2, 54, 19, 507, 2, 118, 475, 1, 87, 1, 3, 835, 778, 22, 464, 1, 10, 79, 545, 11, 23, 12, 105, 991, 59, 464, 384, 37, 21, 597, 33, 12, 11, 23, 1, 212, 10, 79, 1, 64, 914, 230, 55, 69, 1, 259, 506, 114, 5, 2, 11, 385, 161, 5, 102, 144, 3, 721, 1, 107, 91, 3, 497, 1, 97, 15, 92, 9, 2, 15, 497, 221, 42, 172, 2, 134, 497, 51, 20, 721, 6, 57, 40]\n",
            "\n",
            "X_train_sequences data type: <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max training sequence length\n",
        "maxlenCalc = max([len(x) for x in X_train_sequences])\n",
        "print(maxlenCalc) # = 814"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSmlJzznMh2S",
        "outputId": "76597919-8c90-4175-9b95-50334c96e67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set some restrictive limits to make it fast for now\n",
        "#max_features = 500\n",
        "max_len = 180 #replace with: maxlenCalc = max([len(x) for x in X_train_sequences])\n",
        "              #or replace with desired length of dim divided by 50 (length of vectors per word)\n",
        "padding_type='post'\n",
        "truncation_type='post'"
      ],
      "metadata": {
        "id": "j58MX0WVMkM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding or truncating the lists (sequences) to make equal length. Now Numpy Arrays\n",
        "# replace with maxlenCalc (887), but keep at 300 for now for speed..\n",
        "X_train_SeqPad = pad_sequences(X_train_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "# ---\n",
        "X_testN0_SeqPad = pad_sequences(X_testN0_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "X_testN1_SeqPad = pad_sequences(X_testN1_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "X_testN2_SeqPad = pad_sequences(X_testN2_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "X_testN3_SeqPad = pad_sequences(X_testN3_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n"
      ],
      "metadata": {
        "id": "51uknzIwR5qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the results to inspect what is happening\n",
        "print(\"\\nPadded training sequences of first doc:\\n\", X_train_SeqPad[0])\n",
        "print(\"\\nX_train_SeqPad data type:\", type(X_train_SeqPad))\n",
        "print(\"\\nX_train_SeqPad shape:\", X_train_SeqPad.shape) #rows (67625) by maxlen (100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skRLUWGoMsbT",
        "outputId": "992e1ff0-eb9f-4de3-b878-af96205ca35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded training sequences of first doc:\n",
            " [474   4  49 442 271  10   8 258   2  51   8 967  53   5 833  51   6   4\n",
            "   2   1 968   3 865 721  36  11  97  22 249 146 834  69 866   5  51 411\n",
            "  69 596   2 866   1   5 644  12 521   2  16 721  30  24 691 505  77 197\n",
            "  45  16   1  97  15   1 188 442 188 531 442  38  21 753  69   1 506 114\n",
            "   3  11 708   6 195 721   6  30 737 432 738  24   1 153   2 200   2 497\n",
            "   6  40 286 777  33  25  22 355   2 967 189  18 155   5 111   5   1   3\n",
            "  31 362 139   9   7 362  38  79  21   5   1 109  38  21 941   2 721   8\n",
            "   1 812   3   4 362   7  34  11 813 334   1   1 271 103  86 196   1 223\n",
            " 969   1 402 867  43  22  15  12  38 970 355  87  97   8 531   5 721   2\n",
            " 134  40  87   1 866 335   3   1 269 223  98  22  64 721   2  54  19 507]\n",
            "\n",
            "X_train_SeqPad data type: <class 'numpy.ndarray'>\n",
            "\n",
            "X_train_SeqPad shape: (1000, 180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText > Utils Reduce Method (worked)"
      ],
      "metadata": {
        "id": "Wyt3qxWIY7LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fasttext_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/cc.en.300.bin'"
      ],
      "metadata": {
        "id": "qy0xv_hMKiRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fasttext==0.6.0"
      ],
      "metadata": {
        "id": "lnDXGPvmW3XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ft = fasttext.load_model(urlopen(\"https://drive.google.com/file/d/1f294foDnmwJs0t008opDikTJoWEHtNzr/view?usp=sharing\"))\n",
        "#ft = fasttext.load_model(\"https://drive.google.com/file/d/1f294foDnmwJs0t008opDikTJoWEHtNzr/view?usp=sharing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "XepK6bf1aMMB",
        "outputId": "8590d308-62c0-4c00-81a9-c3b4b6c13cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-aa87bdaeb754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#ft = fasttext.load_model(urlopen(\"https://drive.google.com/file/d/1f294foDnmwJs0t008opDikTJoWEHtNzr/view?usp=sharing\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://drive.google.com/file/d/1f294foDnmwJs0t008opDikTJoWEHtNzr/view?usp=sharing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32mfasttext/fasttext.pyx\u001b[0m in \u001b[0;36mfasttext.fasttext.load_model\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: fastText: trained model cannot be opened!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/cc.en.300.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COTkQEivt1W7",
        "outputId": "c1399705-17cd-46f2-8f1f-1d05e6247a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.util.reduce_model(ft, 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrbfodmFZsp6",
        "outputId": "6b88a32f-5801-42cf-e9dc-625882450c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<fasttext.FastText._FastText at 0x7fcb42c16dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ft.get_dimension())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42RlPWSDWojw",
        "outputId": "b0c019b3-6e7b-4dcb-dccf-4ad8c10c75d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft.save_model(\"FastText_UtilReduced50dim.bin\")"
      ],
      "metadata": {
        "id": "EV_046Pi3va8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_ft = fasttext.load_model(\"FastText_UtilReduced50dim.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q3uW10X5Ai7",
        "outputId": "7366e4bd-3385-4d15-8574-b9afaa124233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_ft.save_model('FastText_UtilReduced50dim.txt')"
      ],
      "metadata": {
        "id": "oQo1szVt5JmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp FastText_UtilReduced50dim.txt /content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/"
      ],
      "metadata": {
        "id": "5XmEZdLz5ppo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Matrix Original"
      ],
      "metadata": {
        "id": "zTDgEVeTmTEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_INDEX\n",
        "#Creating the GloVe Embedding Index (aka embedding dictionary)\n",
        "#This does not use anything else than the GloVe data (words and their vectors)\n",
        "\n",
        "embedding_dim = 50 #based on glovefile..  ('...50d.txt')\n",
        "embeddings_index = {} # == 'embeddings_dictionary' in https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained#5.-Vectorization\n",
        "glovefile = open('/content/drive/MyDrive/MYDATA/glove.6B/glove.6B.50d.txt')\n",
        "for line in glovefile:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "glovefile.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')\n",
        "#note to self: the dim number (50) does not influence the total word vector count (i.e. 400k for either 50 or 100 dims..)"
      ],
      "metadata": {
        "id": "KKKwdU-WMu-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_MATRIX\n",
        "#this is where we need to grab each word from the padded sequences and match them with the embeddings index to create the embedding_matrix\n",
        "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
        "\n",
        "for word, embedding in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[embedding] = embedding_vector\n",
        "        \n",
        "#embedding_matrix #numpy array"
      ],
      "metadata": {
        "id": "HfMYhTL-MyHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking the padded sequences (word indexes per row), and replacing each index with an array of 50 vectors \n",
        "#(taken from the embedding matrix because that is where the indexes match between train and glove)\n",
        "\n",
        "#this (in this small form) takes up about 2.7gb RAM..\n",
        "\n",
        "listylist = []\n",
        "for x in X_train_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist.append(y)\n",
        "\n",
        "# the test sets:\n",
        "listylist_testN0 = []\n",
        "for x in X_testN0_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN0.append(y)\n",
        "\n",
        "listylist_testN1 = []\n",
        "for x in X_testN1_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN1.append(y)\n",
        "\n",
        "listylist_testN2 = []\n",
        "for x in X_testN2_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN2.append(y)\n",
        "\n",
        "listylist_testN3 = []\n",
        "for x in X_testN3_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN3.append(y)\n",
        "\n",
        "#prior to this block: 3.2 gb\n",
        "#after: 5.9"
      ],
      "metadata": {
        "id": "el6vXINYM0zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nested list to np array (np array is better for use later on)\n",
        "#another 2 gb of ram\n",
        "nparraylist_train = np.array(listylist)\n",
        "# ---\n",
        "nparraylist_testN0 = np.array(listylist_testN0)\n",
        "nparraylist_testN1 = np.array(listylist_testN1)\n",
        "nparraylist_testN2 = np.array(listylist_testN2)\n",
        "nparraylist_testN3 = np.array(listylist_testN3)\n",
        "#prior to this block: 5.9 gb\n",
        "#after: 8.2"
      ],
      "metadata": {
        "id": "SVWtII0wM7IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape parameters (3dim array to 2dim array)\n",
        "\n",
        "lenDim1_train = len(nparraylist_train) #67625\n",
        "lenDim1_testN0 = len(nparraylist_testN0) #684\n",
        "\n",
        "lenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #5000\n",
        "lenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #5000"
      ],
      "metadata": {
        "id": "vs8cKW_6M_sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#however these are 3 dims, and need to be 2.. (1 rows, 2nd the array of vectors..)\n",
        "# this can be done with reshape (see below)\n",
        "\n",
        "X_train_GloVe = np.reshape(\n",
        "               nparraylist_train,     # the array to be reshaped\n",
        "               (lenDim1_train, lenDim2_train)  # dimensions of the new array\n",
        "              )\n",
        "\n",
        "# --- (change to for loop to get these 4 sets reshaped in one block)\n",
        "X_testN0_GloVe = np.reshape(\n",
        "               nparraylist_testN0,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # dims of test\n",
        "              )\n",
        "\n",
        "X_testN1_GloVe = np.reshape(\n",
        "               nparraylist_testN1,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\n",
        "              )\n",
        "\n",
        "X_testN2_GloVe = np.reshape(\n",
        "               nparraylist_testN2,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  \n",
        "              )\n",
        "\n",
        "X_testN3_GloVe = np.reshape(\n",
        "               nparraylist_testN3,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0) \n",
        "              )\n",
        "#prior to this block: 8.2 gb\n",
        "#after: 8.2"
      ],
      "metadata": {
        "id": "70LRpebvNC-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train, embedding_matrix, X_train_SeqPad, X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad, embeddings_index, df_test, word_index, X_train_sequences, X_testN0_sequences, X_testN1_sequences, X_testN2_sequences, X_testN3_sequences = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "#del df_train, X_train, embedding_matrix, X_train_SeqPad, X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad, embeddings_index, df_test, word_index, X_train_sequences, X_testN0_sequences, X_testN1_sequences, X_testN2_sequences, X_testN3_sequences"
      ],
      "metadata": {
        "id": "oMrIy7I1NJ77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Through Gensim (failed)"
      ],
      "metadata": {
        "id": "kLFh57qscStc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim"
      ],
      "metadata": {
        "id": "7M9Qw0ShdkHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fasttext_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/cc.en.300.bin'"
      ],
      "metadata": {
        "id": "rsDbiWfcc484"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim==3.8.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AftfOcQ0dFol",
        "outputId": "c923264a-4b89-44fe-ea28-fe1fd8032d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.8/dist-packages (3.8.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from gensim==3.8.3) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import load_facebook_model"
      ],
      "metadata": {
        "id": "rLHfwZaEcWs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext = load_facebook_model(fasttext_path)\n",
        "#model = KeyedVectors.load_facebook_vectors(fasttext_path)\n",
        ">>> import fasttext.util\n",
        ">>> fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        ">>> ft = fasttext.load_model('cc.en.300.bin')"
      ],
      "metadata": {
        "id": "WuV3zVtMeY9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastText()\n",
        "model.load_model('/path/to/model.bin')"
      ],
      "metadata": {
        "id": "tl6e9i-gcUS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My own method"
      ],
      "metadata": {
        "id": "IOswuLauiQcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_INDEX\n",
        "#Creating the GloVe Embedding Index (aka embedding dictionary)\n",
        "#This does not use anything else than the GloVe data (words and their vectors)\n",
        "\n",
        "embedding_dim = 100 #based on glovefile..  ('...50d.txt')\n",
        "embeddings_index = {} # == 'embeddings_dictionary' in https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained#5.-Vectorization\n",
        "wiki_ft_file = open('/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/model.txt')\n",
        "for line in wiki_ft_file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "wiki_ft_file.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')\n",
        "#note to self: the dim number (50) does not influence the total word vector count (i.e. 400k for either 50 or 100 dims..)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "hP7yq4rpiSoZ",
        "outputId": "cfc618f7-f2cc-4c0d-8239-ff4e73f4150b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-bcbb4677e582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# == 'embeddings_dictionary' in https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained#5.-Vectorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mwiki_ft_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/model.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwiki_ft_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd1 in position 681: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#similar to mine, but more elegant:\n",
        "#https://datascience.stackexchange.com/questions/20071/how-do-i-load-fasttext-pretrained-model-with-gensim\n",
        "\"\"\"\n",
        "from gensim.models import FastText\n",
        "def load_fasttext():\n",
        "        print('loading word embeddings...')\n",
        "        embeddings_index = {}\n",
        "        f = open('../input/fasttext/wiki.simple.vec',encoding='utf-8')\n",
        "        for line in tqdm(f):\n",
        "        values = line.strip().rsplit(' ')\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        print('found %s word vectors' % len(embeddings_index))\n",
        "    \n",
        "        return embeddings_index\n",
        "\n",
        "embeddings_index=load_fastext()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TUDe48tIk5kN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_MATRIX\n",
        "#this is where we need to grab each word from the padded sequences and match them with the embeddings index to create the embedding_matrix\n",
        "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
        "\n",
        "for word, embedding in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[embedding] = embedding_vector\n",
        "        \n",
        "#embedding_matrix #numpy array"
      ],
      "metadata": {
        "id": "jHZM3Oz0i_v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking the padded sequences (word indexes per row), and replacing each index with an array of 50 vectors \n",
        "#(taken from the embedding matrix because that is where the indexes match between train and glove)\n",
        "\n",
        "#this (in this small form) takes up about 2.7gb RAM..\n",
        "\n",
        "listylist = []\n",
        "for x in X_train_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist.append(y)\n",
        "\n",
        "# the test sets:\n",
        "listylist_testN0 = []\n",
        "for x in X_testN0_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN0.append(y)\n",
        "\n",
        "listylist_testN1 = []\n",
        "for x in X_testN1_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN1.append(y)\n",
        "\n",
        "listylist_testN2 = []\n",
        "for x in X_testN2_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN2.append(y)\n",
        "\n",
        "listylist_testN3 = []\n",
        "for x in X_testN3_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN3.append(y)\n",
        "\n",
        "#prior to this block: 3.2 gb\n",
        "#after: 5.9"
      ],
      "metadata": {
        "id": "_YlJomHMjC6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nested list to np array (np array is better for use later on)\n",
        "#another 2 gb of ram\n",
        "nparraylist_train = np.array(listylist)\n",
        "# ---\n",
        "nparraylist_testN0 = np.array(listylist_testN0)\n",
        "nparraylist_testN1 = np.array(listylist_testN1)\n",
        "nparraylist_testN2 = np.array(listylist_testN2)\n",
        "nparraylist_testN3 = np.array(listylist_testN3)\n",
        "#prior to this block: 5.9 gb\n",
        "#after: 8.2"
      ],
      "metadata": {
        "id": "zt9VdvzOjVLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape parameters (3dim array to 2dim array)\n",
        "\n",
        "lenDim1_train = len(nparraylist_train) #67625\n",
        "lenDim1_testN0 = len(nparraylist_testN0) #684\n",
        "\n",
        "lenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #5000\n",
        "lenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #5000"
      ],
      "metadata": {
        "id": "Pqtuv9IIjYTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#however these are 3 dims, and need to be 2.. (1 rows, 2nd the array of vectors..)\n",
        "# this can be done with reshape (see below)\n",
        "\n",
        "X_train_GloVe = np.reshape(\n",
        "               nparraylist_train,     # the array to be reshaped\n",
        "               (lenDim1_train, lenDim2_train)  # dimensions of the new array\n",
        "              )\n",
        "\n",
        "# --- (change to for loop to get these 4 sets reshaped in one block)\n",
        "X_testN0_GloVe = np.reshape(\n",
        "               nparraylist_testN0,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # dims of test\n",
        "              )\n",
        "\n",
        "X_testN1_GloVe = np.reshape(\n",
        "               nparraylist_testN1,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\n",
        "              )\n",
        "\n",
        "X_testN2_GloVe = np.reshape(\n",
        "               nparraylist_testN2,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  \n",
        "              )\n",
        "\n",
        "X_testN3_GloVe = np.reshape(\n",
        "               nparraylist_testN3,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0) \n",
        "              )\n",
        "#prior to this block: 8.2 gb\n",
        "#after: 8.2"
      ],
      "metadata": {
        "id": "7kc_olHOjbYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}