{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jNlaWciU8jKd",
        "fGGHxRy1gjgH",
        "lq3nJuzbLgvx",
        "DeWsxj7uL_xN",
        "1VN1NfeRygya",
        "nUVQqCY3Xqba",
        "zbfBs6SsoiRk",
        "OG84ZsAgHXRg",
        "sg6tCmf5weJf",
        "gL18k159KePF",
        "JdSQhnhUNJYY",
        "F0nj56E2Eb1A",
        "ythDSEFqMIfw",
        "srCAridl0STZ",
        "W-88O9XEEtQQ",
        "mPlTuQSZCBsb",
        "M7r-wRjyzMwd",
        "GloPNlsOd99p",
        "pG-N5mm8n42R",
        "jt6KkLPeQwQZ",
        "oqR950o0vcNk"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNeIYU8KqbQeMmn0pxaQ8ma",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenPeutz/Masterthesis-Disinformation-NLP/blob/master/MAIN_Embeddings%2BModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this Notebook\n",
        "This is the full notebook <sup>1</sup> that provides all embeddings and models compaired throughout this project. This means a 6 x 7 x 4 design in terms of embeddings, models and testsets resulting in 42 NLP architectures, each tested on 4 noise<sup>2</sup> levels.   \n",
        "<br>\n",
        "\n",
        "**Embeddings:**\n",
        "- CountVectorizer (BoW)\n",
        "- HashingVector (HV)\n",
        "- TF-IDF\n",
        "- GloVe\n",
        "- Word2Vec (w2v)\n",
        "- FastText (ft)\n",
        "<br><br>\n",
        "\n",
        "**Classification Models**\n",
        "- Logistic Regression (LR)\n",
        "- Naive Bayes (NB)\n",
        "- Random Forest (RF)\n",
        "- Support Vector Machine (SVM)\n",
        "- K-Nearest Neighbour (KNN)\n",
        "- GradientBoosting (GB)\n",
        "- Extreme Gradient Boosting (XGB)   \n",
        "  \n",
        "<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<br>\n",
        "  \n",
        "<sup>1  </sup>*In order for all this to fit within the 35GB RAM bounds of the google colab environment, the pretrained embeddings (GloVe, w2v and fasttext) used in this notebook have undergone three restrictions (1.reduced to 50 dimensions per word (for w2v and fasttext this was done through CPA), 2. the 'maxlength' of each doc has been reduced to 80 tokens. and 3. the train set used for training takes a sample of 20k docs instead of the full 51k).  Full embeddings, models and their results can be found in the separate notebooks.* *italicised text*    \n",
        "<br>\n",
        "  \n",
        "\n",
        "<sup>2  </sup>*Noise levels as introduced by machine backtranslations. 'N0' being the original testset (the version similar to training), 'N1' (1 level of backtranslation (EN -> RU -> EN), continuing to 'N3'.\n",
        "For rough assessment of noise in a purely lexical sense, the Jaro-Winkler Distances (normalized) have been calculated and imported before and are imported in this notebook*. "
      ],
      "metadata": {
        "id": "_ff73cC1qSeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import + mounting drive"
      ],
      "metadata": {
        "id": "jNlaWciU8jKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d39fJF8Pajnc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse.csc import csc_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#preprocessing:\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "#from keras_preprocessing.sequence import pad_sequences  #  <- commented out for now as it gives an error but worked in the past and is needed for glove..\n",
        "# https://stackoverflow.com/questions/72326025/cannot-import-name-pad-sequences-from-keras-preprocessing-sequence  \n",
        "  #try:\n",
        "from keras.utils import pad_sequences#or: from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "#import nltk\n",
        "\n",
        "\n",
        "#pickle:\n",
        "from pickle import dump, load\n",
        "picklepath = '/content/drive/MyDrive/MYDATA/PickledModels/'\n",
        "\n",
        "#classifiers:\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "#evaluation\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "#system:\n",
        "import gc\n",
        "import sys\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AF-1NZQ7kivh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e5579f-49c6-4603-f47b-ef0ff08b4e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#no need to run this everytime\n",
        "\n",
        "#!pip install pipreqs\n",
        "#!pipreqs . "
      ],
      "metadata": {
        "id": "PvCl6kNaY_eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this colab builds on the FULL_CREATION_DataSet.ipynb colab\n",
        "# link: https://colab.research.google.com/drive/1lFMu0WaWkpDCZpPCH3eC3LJWlYNlbg06?usp=sharing\n",
        "# this previous colab exports the seperate train and test sets.."
      ],
      "metadata": {
        "id": "b9eyrDrPKs48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading in test & train sets (csv method)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fGGHxRy1gjgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathTESTcsv = '/content/drive/MyDrive/MYDATA/full_TEST01_NX.csv' #be mindful, this is Test-INC (as the INC is the one with the translation columns)\n",
        "pathTRAINcsv = '/content/drive/MyDrive/MYDATA/df_full2_TRAIN30split.csv' #NTS: this trainset is not capped at 2800 chars.. (the testset is..)\n",
        "\n",
        "df_test = pd.read_csv(pathTESTcsv)\n",
        "df_train = pd.read_csv(pathTRAINcsv)"
      ],
      "metadata": {
        "id": "SZmikDDHgswG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "X29e4itUidiZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0776ba88-bcb3-4cad-c831-f2a243d38a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label  \\\n",
              "0  Trump s weekend trips to Mar-a-Lago and his ne...      1   \n",
              "1  The British Parliament conducted what can only...      1   \n",
              "2  The United States and China will work together...      0   \n",
              "3  A federal judge in Hawaii issued a nationwide ...      0   \n",
              "4  Obama made Wall Street out to be the enemy dur...      1   \n",
              "\n",
              "                                             text_N1  \\\n",
              "0  Trump's weekend trips to Mar-a-Lago and its ne...   \n",
              "1  The British Parliament held what can only be d...   \n",
              "2  The United States and China will work together...   \n",
              "3  On Wednesday night, a federal judge in Hawaii ...   \n",
              "4  Obama made Wall Street his enemy during his pr...   \n",
              "\n",
              "                                             text_N2  \\\n",
              "0  Trump's weekend trips to Mar-a-Lago and the ne...   \n",
              "1  The British Parliament held what can only be d...   \n",
              "2  The United States and China will work together...   \n",
              "3  On Wednesday night, a federal judge in Hawaii ...   \n",
              "4  Obama made Wall Street his enemy during his pr...   \n",
              "\n",
              "                                             text_N3  \n",
              "0  Trump's weekend trips to Mar-a-Lago and the ne...  \n",
              "1  The British Parliament staged what can only be...  \n",
              "2  The United States and China will work together...  \n",
              "3  On Wednesday night, a federal judge in Hawaii ...  \n",
              "4  Obama made Wall Street his enemy during his pr...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24d318a4-48d3-491e-8c66-902505bf09a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>text_N1</th>\n",
              "      <th>text_N2</th>\n",
              "      <th>text_N3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Trump s weekend trips to Mar-a-Lago and his ne...</td>\n",
              "      <td>1</td>\n",
              "      <td>Trump's weekend trips to Mar-a-Lago and its ne...</td>\n",
              "      <td>Trump's weekend trips to Mar-a-Lago and the ne...</td>\n",
              "      <td>Trump's weekend trips to Mar-a-Lago and the ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The British Parliament conducted what can only...</td>\n",
              "      <td>1</td>\n",
              "      <td>The British Parliament held what can only be d...</td>\n",
              "      <td>The British Parliament held what can only be d...</td>\n",
              "      <td>The British Parliament staged what can only be...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The United States and China will work together...</td>\n",
              "      <td>0</td>\n",
              "      <td>The United States and China will work together...</td>\n",
              "      <td>The United States and China will work together...</td>\n",
              "      <td>The United States and China will work together...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A federal judge in Hawaii issued a nationwide ...</td>\n",
              "      <td>0</td>\n",
              "      <td>On Wednesday night, a federal judge in Hawaii ...</td>\n",
              "      <td>On Wednesday night, a federal judge in Hawaii ...</td>\n",
              "      <td>On Wednesday night, a federal judge in Hawaii ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Obama made Wall Street out to be the enemy dur...</td>\n",
              "      <td>1</td>\n",
              "      <td>Obama made Wall Street his enemy during his pr...</td>\n",
              "      <td>Obama made Wall Street his enemy during his pr...</td>\n",
              "      <td>Obama made Wall Street his enemy during his pr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24d318a4-48d3-491e-8c66-902505bf09a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24d318a4-48d3-491e-8c66-902505bf09a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24d318a4-48d3-491e-8c66-902505bf09a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#BE AWARE:\n",
        "df_train = df_train.sample(10000, random_state=42) #smaller for faster testing.."
      ],
      "metadata": {
        "id": "baaC0bylv13-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_test[df_test['text_N1'].str.contains('д|и|г')]) #how many rows are not translated back to english correctly? # ±30%..\n",
        "# redo preceeding colab, but with more rigorous cleaning (en see if thats helps), if not, redo with other languaage than russian."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA_2qZgGtDkB",
        "outputId": "2b3d3f90-0848-444e-a4ff-b232b67f45ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "561"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 0: Assigning the series needed for embedding\n",
        "\n",
        "\n",
        "*   X_train\n",
        "*   y_train\n",
        "*   X_testNx (for N0 to N3)\n",
        "*   y_test (same for all N's)"
      ],
      "metadata": {
        "id": "lq3nJuzbLgvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_train['text']\n",
        "y_train = df_train['label']\n",
        "X_testN0 = df_test['text']\n",
        "X_testN1 = df_test['text_N1']\n",
        "X_testN2 = df_test['text_N2']\n",
        "X_testN3 = df_test['text_N3']\n",
        "y_testALL = df_test['label']"
      ],
      "metadata": {
        "id": "3eew7bL3vqXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train[:10]\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "3x-Q9dt2OtFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2fba02a-685c-45e8-983e-301bed3710ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 1: BoW (Countvectorizer)"
      ],
      "metadata": {
        "id": "DeWsxj7uL_xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(min_df=4, max_df=0.8)\n",
        "vectorizer.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpdFQ2u1w8y5",
        "outputId": "051391fd-a611-4a31-88f6-ef78f0df7a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_df=0.8, min_df=4)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_BoW = vectorizer.transform(X_train)\n",
        "# ---\n",
        "X_testN0_BoW  = vectorizer.transform(X_testN0)\n",
        "X_testN1_BoW  = vectorizer.transform(X_testN1)\n",
        "X_testN2_BoW  = vectorizer.transform(X_testN2)\n",
        "X_testN3_BoW  = vectorizer.transform(X_testN3)"
      ],
      "metadata": {
        "id": "5RT_ubuuxCAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.shape # <- 67625 by 1\n",
        "#X_train_BoW.shape # <- 67625 by 60326\n",
        "#type(X_testN0_BoW) # <- sparse matrix\n",
        "#type(X_train) #<- series"
      ],
      "metadata": {
        "id": "E-mvvu_JxYv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#n_features = len(X_train)\n",
        "#print(n_features)"
      ],
      "metadata": {
        "id": "sA5m42yv3hID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 2: HashingVectorizer"
      ],
      "metadata": {
        "id": "1VN1NfeRygya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import HashingVectorizer\n",
        "hashingvectorizer = HashingVectorizer(n_features=60326)\n",
        "hashingvectorizer.fit(X_train)"
      ],
      "metadata": {
        "id": "htuqAwvEvmeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b534bb3c-df7c-4475-c59a-17a1455e5bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HashingVectorizer(n_features=60326)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_HV = hashingvectorizer.transform(X_train)\n",
        "# ---\n",
        "X_testN0_HV  = hashingvectorizer.transform(X_testN0)\n",
        "X_testN1_HV  = hashingvectorizer.transform(X_testN1)\n",
        "X_testN2_HV  = hashingvectorizer.transform(X_testN2)\n",
        "X_testN3_HV  = hashingvectorizer.transform(X_testN3)"
      ],
      "metadata": {
        "id": "mCo4HiF2xXwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS BLOCK IS ONLY FOR THE NAIVE BAYES CLASSIFIER\n",
        "\n",
        "#scaling\n",
        "#minmax would be the go to for sparse data, however our sparse data contains negative values -> MaxAbsScaler\n",
        "\n",
        "#from sklearn.preprocessing import MaxAbsScaler\n",
        "MaxAbs_scaler = MaxAbsScaler()\n",
        "\n",
        "# transform data\n",
        "# (check whether indeed best to fit scaler only on trainset)\n",
        "MaxAbs_scaler.fit(X_train_HV)\n",
        "X_train_HV_scaled = MaxAbs_scaler.transform(X_train_HV)\n",
        "X_testN0_HV_scaled = MaxAbs_scaler.transform(X_testN0_HV)\n",
        "X_testN1_HV_scaled = MaxAbs_scaler.transform(X_testN1_HV)\n",
        "X_testN2_HV_scaled = MaxAbs_scaler.transform(X_testN2_HV)\n",
        "X_testN3_HV_scaled = MaxAbs_scaler.transform(X_testN3_HV)\n",
        "\n",
        "#shift +1 for all non-zero values to be usable for Naive Bayes classifier\n",
        "#from scipy.sparse.csc import csc_matrix\n",
        "X_train_HV_scaled.data += 1\n",
        "X_testN0_HV_scaled.data += 1\n",
        "X_testN1_HV_scaled.data += 1\n",
        "X_testN2_HV_scaled.data += 1\n",
        "X_testN3_HV_scaled.data += 1"
      ],
      "metadata": {
        "id": "M6co0A7UJP0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 3: TF-IDF\n",
        "- Builds on output of BoW countvectorizer (embedding 1)"
      ],
      "metadata": {
        "id": "nUVQqCY3Xqba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_transformer.fit(X_train_BoW)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8u8_LgGXxK9",
        "outputId": "311654a8-6543-4e0a-c716-77fb8ca74949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_TFIDF = tfidf_transformer.transform(X_train_BoW)\n",
        "# ---\n",
        "X_testN0_TFIDF  = tfidf_transformer.transform(X_testN0_BoW)\n",
        "X_testN1_TFIDF  = tfidf_transformer.transform(X_testN1_BoW)\n",
        "X_testN2_TFIDF  = tfidf_transformer.transform(X_testN2_BoW)\n",
        "X_testN3_TFIDF  = tfidf_transformer.transform(X_testN3_BoW)"
      ],
      "metadata": {
        "id": "DsWYSPbTb4fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_testN0_TFIDF.shape # <- 684 by 60326\n",
        "#type(X_testN0_TFIDF) # <- sparse matrix\n",
        "X_testN0_TFIDF.shape"
      ],
      "metadata": {
        "id": "7CMNmoCucnEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c517064-4c3e-43d8-f93b-5617d7b7d493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21999, 21435)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepping for pretraind embeddings\n",
        "- GloVe\n",
        "- w2v\n",
        "- fasttext"
      ],
      "metadata": {
        "id": "zbfBs6SsoiRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#from keras.preprocessing.text import Tokenizer\n",
        "#from keras_preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "ESYnHLhLkcK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separate collab using a small test set to try to work out this problem: https://colab.research.google.com/drive/1Pi55fZS9rlYyouN1bzi47lNkiSB2PzBI?usp=sharing \n",
        "\n",
        "//at this point 2.2GB should be used.\n",
        "\n",
        "\n",
        "For other pretrained embeddings (also 50dim ones):\n",
        "https://developer.syn.co.in/tutorial/bot/oscova/pretrained-vectors.html "
      ],
      "metadata": {
        "id": "kCLuJr1fXtR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.6GB prior to this block\n",
        "word_tokenizer = Tokenizer(num_words = 1000)\n",
        "word_tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "word_index = word_tokenizer.word_index\n",
        "vocab_length = len(word_index) + 1 #cant be unique words recheck this. Probably need cleaning of data first so that e.g. @CNN is not counted as unique word\n",
        "# https://github.com/keras-team/keras/issues/7551\n",
        "#vocab_length -> 267981 / 231677"
      ],
      "metadata": {
        "id": "0jdbm-S8o_K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#list(word_index)[-300:]\n",
        "\n",
        "#as I thought, the vocab can be made lot shorter by:\n",
        "# - cleaning data (removing \" before tokenizing)\n",
        "# - removing @mentions or somehting (perhaps just remove everything immediately following @?)"
      ],
      "metadata": {
        "id": "FuCgyabY2zUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_index.get('trump'))\n",
        "# so this dictionary (word_index) has the vocab (267981 words..) stored sort of in order of usage ('the' is 1, 'a' is 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guijQSp5vwSE",
        "outputId": "32bfc120-2e3b-4c85-d0f6-c8b296c9f7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#convert series to list (prior to this 2.7GB used)\n",
        "X_train_sequences = word_tokenizer.texts_to_sequences(X_train)\n",
        "# ---\n",
        "X_testN0_sequences = word_tokenizer.texts_to_sequences(X_testN0)\n",
        "X_testN1_sequences = word_tokenizer.texts_to_sequences(X_testN1)\n",
        "X_testN2_sequences = word_tokenizer.texts_to_sequences(X_testN2)\n",
        "X_testN3_sequences = word_tokenizer.texts_to_sequences(X_testN3)"
      ],
      "metadata": {
        "id": "bjMh9a2msB0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the results to inspect what is happening\n",
        "print(\"Word index for example word 'home' :\\n\", word_index.get('home')) #chosen as it is the 1st word in X_train 1st doc\n",
        "print(\"\\nTraining sequence of first doc:\\n\", X_train_sequences[0]) #first doc, starting with word 'home'\n",
        "print(\"\\nX_train_sequences data type:\", type(X_train_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BmM2l5jyv_c",
        "outputId": "66b80177-871c-4e70-b50f-a31b54206d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index for example word 'home' :\n",
            " 282\n",
            "\n",
            "Training sequence of first doc:\n",
            " [70, 17, 5, 20, 346, 29, 127, 2, 39, 59, 32, 35, 28, 139, 11, 21, 587, 6, 200, 365, 756, 6, 1, 229, 139, 12, 21, 43, 1, 282, 3, 4, 564, 138, 999, 6, 14, 1, 96, 169, 40, 95, 28, 9, 1, 2, 47, 354, 103, 1, 35, 1, 52, 16, 1, 117, 10, 4, 465, 5, 381, 24, 2, 240, 153, 8, 47, 205, 5, 28, 310, 9, 4, 1, 16, 6, 5, 19, 1, 8, 1, 5, 5, 16, 19, 30, 24, 33, 70, 17, 38, 25, 23, 1, 8, 1, 5, 7, 11, 4, 221, 392, 59, 892, 362, 6, 52, 241, 1, 2, 47, 31, 9, 10, 2, 17, 2, 1, 73, 273, 674, 9, 7, 12, 9, 25, 6, 1, 593, 6, 77, 684, 17, 72, 196, 861, 419, 70, 17, 236, 25, 2, 23, 6, 4, 7, 9, 4, 381, 14, 49, 3, 9, 1, 98, 117, 40, 12, 37, 62, 23, 2, 40, 95, 139, 17, 37, 23, 14, 76, 5, 7, 74, 236, 25, 159, 17, 9, 4, 960, 31, 2, 102, 5, 553, 2, 635, 757, 24, 1, 121, 48, 37, 23, 30, 2, 153, 11, 4, 370, 27, 1, 520, 1, 18, 200, 11, 976, 5, 12, 11, 2, 130, 7, 17, 9, 25, 537, 31, 38, 131, 45, 175, 2, 735, 73, 451, 11, 729, 1, 24, 17, 5, 20, 346, 6, 320, 232, 174]\n",
            "\n",
            "X_train_sequences data type: <class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get max training sequence length\n",
        "maxlenCalc = max([len(x) for x in X_train_sequences])\n",
        "print(maxlenCalc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGEv7Sra0RO4",
        "outputId": "37257b4c-0842-4fca-fa6a-a18f49adf1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#set some restrictive limits to make it fast for now\n",
        "#max_features = 500\n",
        "max_len = 80 #replace with: maxlenCalc = max([len(x) for x in X_train_sequences])\n",
        "              #or replace with desired length of dim divided by 50 (length of vectors per word)\n",
        "              #For now limited to a mere 120 to save RAM in order to fit all in a single 32GB RAM environment\n",
        "padding_type='post'\n",
        "truncation_type='post'"
      ],
      "metadata": {
        "id": "RnuOnnDvjZ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding or truncating the lists (sequences) to make equal length. Now Numpy Arrays\n",
        "# replace with maxlenCalc (887), but keep at 300 for now for speed..\n",
        "X_train_SeqPad = pad_sequences(X_train_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "# ---\n",
        "X_testN0_SeqPad = pad_sequences(X_testN0_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "X_testN1_SeqPad = pad_sequences(X_testN1_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "X_testN2_SeqPad = pad_sequences(X_testN2_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "X_testN3_SeqPad = pad_sequences(X_testN3_sequences, maxlen=max_len, padding=padding_type, truncating=truncation_type)\n",
        "\n",
        "#after this block: 3.0gb"
      ],
      "metadata": {
        "id": "I7E74oqfrpgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the results to inspect what is happening\n",
        "print(\"\\nPadded training sequences of first doc:\\n\", X_train_SeqPad[0])\n",
        "print(\"\\nX_train_SeqPad data type:\", type(X_train_SeqPad))\n",
        "print(\"\\nX_train_SeqPad shape:\", X_train_SeqPad.shape) #rows (67625) by maxlen (110)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB4_voDG1DZ5",
        "outputId": "4de7aafd-3ccc-4eab-d401-8fc591affeb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded training sequences of first doc:\n",
            " [ 70  17   5  20 346  29 127   2  39  59  32  35  28 139  11  21 587   6\n",
            " 200 365 756   6   1 229 139  12  21  43   1 282   3   4 564 138 999   6\n",
            "  14   1  96 169  40  95  28   9   1   2  47 354 103   1  35   1  52  16\n",
            "   1 117  10   4 465   5 381  24   2 240 153   8  47 205   5  28 310   9\n",
            "   4   1  16   6   5  19   1   8]\n",
            "\n",
            "X_train_SeqPad data type: <class 'numpy.ndarray'>\n",
            "\n",
            "X_train_SeqPad shape: (10000, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 4: GloVe"
      ],
      "metadata": {
        "id": "OG84ZsAgHXRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_INDEX\n",
        "\n",
        "# 4GB RAM used prior to this point\n",
        "#Creating the GloVe Embedding Index (aka embedding dictionary)\n",
        "#This does not use anything else than the GloVe data (words and their vectors)\n",
        "\n",
        "embedding_dim = 50 #based on glovefile..  ('...50d.txt')\n",
        "embeddings_index = {} # == 'embeddings_dictionary' in https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained#5.-Vectorization\n",
        "glovefile = open('/content/drive/MyDrive/MYDATA/glove.6B/glove.6B.50d.txt')\n",
        "for line in glovefile:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "glovefile.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')\n",
        "#note to self: the dim number (50) does not influence the total word vector count (i.e. 400k for either 50 or 100 dims..)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vH_1mGOHjixc",
        "outputId": "97562c29-b474-4d55-b87b-45438a939745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#type(embeddings_index) # <- dict\n",
        "print(embeddings_index[\"trump\"])\n",
        "#len(embeddings_index[\"trump\"]) # =50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7agG7cpyiAV",
        "outputId": "c11cdf57-5f0b-473b-81d9-926fe4451304"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.45769    0.85191    0.31098    1.3478    -0.021703  -0.05467\n",
            " -0.17405   -0.71014   -0.57441    0.49786   -0.89382    0.58661\n",
            " -0.77013    0.23779    0.37652    1.3668     0.34667    0.082452\n",
            "  0.17561    0.056062   0.50818    0.33751   -0.8517     0.034619\n",
            " -0.57237   -1.1534    -0.54159    0.24328    0.018737  -0.58645\n",
            "  0.15261    0.53372   -0.27601    0.065097   0.3968    -0.62892\n",
            "  0.0046726  0.081347  -0.32579   -0.60933   -0.36464    0.17591\n",
            "  0.0050718  1.3606    -0.78583    0.083121  -0.75184   -0.74226\n",
            "  0.77053    0.26394  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_MATRIX\n",
        "#this is where we need to grab each word from the padded sequences and match them with the embeddings index to create the embedding_matrix\n",
        "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
        "\n",
        "for word, embedding in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[embedding] = embedding_vector\n",
        "        \n",
        "#embedding_matrix #numpy array"
      ],
      "metadata": {
        "id": "XRNmGxPBrDrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tui11UM7mcZt",
        "outputId": "2ee40c7f-f7ca-4447-e74a-6ba487b1dbb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,\n",
              "        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,\n",
              "       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,\n",
              "       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,\n",
              "        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,\n",
              "       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,\n",
              "       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,\n",
              "       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,\n",
              "       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,\n",
              "        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,\n",
              "       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,\n",
              "       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,\n",
              "       -1.15139998e-01, -7.85809994e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taking the padded sequences (word indexes per row), and replacing each index with an array of 50 vectors \n",
        "#(taken from the embedding matrix because that is where the indexes match between train and glove)\n",
        "\n",
        "#this (in this small form) takes up about 2.7gb RAM..\n",
        "\n",
        "\"\"\"\n",
        "listylist = []\n",
        "for x in X_train_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist.append(y)\n",
        "\n",
        "# the test sets:\n",
        "listylist_testN0 = []\n",
        "for x in X_testN0_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN0.append(y)\n",
        "\n",
        "listylist_testN1 = []\n",
        "for x in X_testN1_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN1.append(y)\n",
        "\n",
        "listylist_testN2 = []\n",
        "for x in X_testN2_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN2.append(y)\n",
        "\n",
        "listylist_testN3 = []\n",
        "for x in X_testN3_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN3.append(y)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#prior to this block: 3.2 gb\n",
        "#after: 5.9"
      ],
      "metadata": {
        "id": "_0Dnm5urqiQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1135963-2ac5-4322-e62e-cf1d7cf93e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlistylist = []\\nfor x in X_train_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist.append(y)\\n\\n# the test sets:\\nlistylist_testN0 = []\\nfor x in X_testN0_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN0.append(y)\\n\\nlistylist_testN1 = []\\nfor x in X_testN1_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN1.append(y)\\n\\nlistylist_testN2 = []\\nfor x in X_testN2_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN2.append(y)\\n\\nlistylist_testN3 = []\\nfor x in X_testN3_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN3.append(y)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nested list to np array (np array is better for use later on)\n",
        "#another 2 gb of ram\n",
        "\"\"\"\n",
        "nparraylist_train = np.array(listylist)\n",
        "# ---\n",
        "nparraylist_testN0 = np.array(listylist_testN0)\n",
        "nparraylist_testN1 = np.array(listylist_testN1)\n",
        "nparraylist_testN2 = np.array(listylist_testN2)\n",
        "nparraylist_testN3 = np.array(listylist_testN3)\n",
        "\"\"\"\n",
        "#prior to this block: 8.2 gb\n",
        "#after: 8.2"
      ],
      "metadata": {
        "id": "UKO_uCQYrpUf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620fe332-062e-451d-945e-2038e441b3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnparraylist_train = np.array(listylist)\\n# ---\\nnparraylist_testN0 = np.array(listylist_testN0)\\nnparraylist_testN1 = np.array(listylist_testN1)\\nnparraylist_testN2 = np.array(listylist_testN2)\\nnparraylist_testN3 = np.array(listylist_testN3)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting lists to free up ram. Somewhow overwriting with zero work better than delete..\n",
        "\"\"\"\n",
        "listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3  = [], [], [], [], []\n",
        "del listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3\n",
        "\n",
        "#import gc\n",
        "gc.collect()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CayW91Q3urvS",
        "outputId": "6a99692e-23a0-40ff-f47a-e355dbdad900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlistylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3  = [], [], [], [], []\\ndel listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3\\n\\n#import gc\\ngc.collect()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#len(nparraylist_train) # = 67625\n",
        "#len(nparraylist_train[0]) # = 100\n",
        "#len(nparraylist_train[0][0]) # = 50"
      ],
      "metadata": {
        "id": "S3nwl8UNsogg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape parameters (3dim array to 2dim array)\n",
        "\"\"\"\n",
        "lenDim1_train = len(nparraylist_train) #51331\n",
        "lenDim1_testN0 = len(nparraylist_testN0) #684\n",
        "\n",
        "lenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #5000\n",
        "lenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #5000\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "WnDQTcGVr0k5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17730f38-304d-4635-f91f-59fc51c042cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlenDim1_train = len(nparraylist_train) #51331\\nlenDim1_testN0 = len(nparraylist_testN0) #684\\n\\nlenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #5000\\nlenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #5000\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#however these are 3 dims, and need to be 2.. (1 rows, 2nd the array of vectors..)\n",
        "# this can be done with reshape (see below)\n",
        "\"\"\"\n",
        "X_train_GloVe = np.reshape(\n",
        "               nparraylist_train,     # the array to be reshaped\n",
        "               (lenDim1_train, lenDim2_train)  # dimensions of the new array\n",
        "              )\n",
        "\n",
        "# --- (change to for loop to get these 4 sets reshaped in one block)\n",
        "X_testN0_GloVe = np.reshape(\n",
        "               nparraylist_testN0,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # dims of test\n",
        "              )\n",
        "\n",
        "X_testN1_GloVe = np.reshape(\n",
        "               nparraylist_testN1,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\n",
        "              )\n",
        "\n",
        "X_testN2_GloVe = np.reshape(\n",
        "               nparraylist_testN2,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  \n",
        "              )\n",
        "\n",
        "X_testN3_GloVe = np.reshape(\n",
        "               nparraylist_testN3,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0) \n",
        "              )\n",
        "\"\"\"\n",
        "#prior to this block: 8.2 gb\n",
        "#after: 8.2"
      ],
      "metadata": {
        "id": "bTKYbjjtrwIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941e0482-bf36-400e-a3b3-1dc101fa129b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_train_GloVe = np.reshape(\\n               nparraylist_train,     # the array to be reshaped\\n               (lenDim1_train, lenDim2_train)  # dimensions of the new array\\n              )\\n\\n# --- (change to for loop to get these 4 sets reshaped in one block)\\nX_testN0_GloVe = np.reshape(\\n               nparraylist_testN0,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  # dims of test\\n              )\\n\\nX_testN1_GloVe = np.reshape(\\n               nparraylist_testN1,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\\n              )\\n\\nX_testN2_GloVe = np.reshape(\\n               nparraylist_testN2,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  \\n              )\\n\\nX_testN3_GloVe = np.reshape(\\n               nparraylist_testN3,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0) \\n              )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the above 5 blocks:\n",
        "# this block jumps from 4GB to 15.5GB..\n",
        "\n",
        "def embedding_generator(X, embedding_matrix):\n",
        "    for x in X:\n",
        "        yield embedding_matrix[x]\n",
        "#(the generator function will be reused for wrod2vec and fasttext)\n",
        "\n",
        "# training set\n",
        "nparraylist_train = np.array(list(embedding_generator(X_train_SeqPad, embedding_matrix)))\n",
        "\n",
        "# test sets\n",
        "test_sets = [X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad]\n",
        "nparraylist_tests = []\n",
        "for X_test in test_sets:\n",
        "    nparraylist_tests.append(np.array(list(embedding_generator(X_test, embedding_matrix))))\n",
        "\n",
        "# reshape arrays\n",
        "dims = (len(nparraylist_train), len(nparraylist_train[0]) * len(nparraylist_train[0][0]))\n",
        "X_train_GloVe = np.reshape(nparraylist_train, dims)\n",
        "\n",
        "X_tests_GloVe = []\n",
        "for nparraylist_test in nparraylist_tests:\n",
        "    dims = (len(nparraylist_test), len(nparraylist_test[0]) * len(nparraylist_test[0][0]))\n",
        "    X_tests_GloVe.append(np.reshape(nparraylist_test, dims))\n",
        "\n",
        "X_testN0_GloVe, X_testN1_GloVe, X_testN2_GloVe, X_testN3_GloVe = X_tests_GloVe\n"
      ],
      "metadata": {
        "id": "CgWyU2E-hlSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#type(nparraylist_train) #numpy.ndarray\n",
        "#nparraylist_train.shape # (51331, 180, 50)\n",
        "np.array_equal(X_testN0_GloVe, X_testN1_GloVe, equal_nan=False) #False, so that seems correct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2H9JfkzfHzv",
        "outputId": "36d63f73-2a44-496b-a799-541c9b41a34a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14.8RAM\n",
        "#viewing if indeed corret:\n",
        "\n",
        "#len(nparraylist_tests[3]) #21999. (0 (N0), 1 (N1), 2 (N2),  and 3 (N3))\n",
        "#nparraylist_tests[3].shape"
      ],
      "metadata": {
        "id": "vNcMZQERdAux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3 = [], [], [], [], []\n",
        "del nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QF9qOiWUqa5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7583627d-8b5d-42eb-b8df-5a7a0502a143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3 = [], [], [], [], []\\ndel nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df_train, embedding_matrix, X_train_SeqPad, X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad, embeddings_index, df_test, word_index, X_train_sequences, X_testN0_sequences, X_testN1_sequences, X_testN2_sequences, X_testN3_sequences = [], [], [], [], [], [], [], [], [], [], [], [], [], [], []\n",
        "#del df_train, X_train, embedding_matrix, X_train_SeqPad, X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad, embeddings_index, df_test, word_index, X_train_sequences, X_testN0_sequences, X_testN1_sequences, X_testN2_sequences, X_testN3_sequences"
      ],
      "metadata": {
        "id": "zI2PQNUOqocp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gc.collect()"
      ],
      "metadata": {
        "id": "fUzdKamvsMnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS BLOCK IS ONLY FOR THE NAIVE BAYES CLASSIFIER\n",
        "\n",
        "#scaling\n",
        "#minmax would be the go to for sparse data, however our sparse data contains negative values -> MaxAbsScaler\n",
        "#from sklearn.preprocessing import MaxAbsScaler\n",
        "#THis takes another 10GB...\n",
        "\n",
        "MaxAbs_scaler = MaxAbsScaler()\n",
        "\n",
        "# transform data\n",
        "# (check whether indeed best to fit scaler only on trainset)\n",
        "MaxAbs_scaler.fit(X_train_GloVe)\n",
        "X_train_GloVe_scaled = MaxAbs_scaler.transform(X_train_GloVe)\n",
        "X_testN0_GloVe_scaled = MaxAbs_scaler.transform(X_testN0_GloVe)\n",
        "X_testN1_GloVe_scaled = MaxAbs_scaler.transform(X_testN1_GloVe)\n",
        "X_testN2_GloVe_scaled = MaxAbs_scaler.transform(X_testN2_GloVe)\n",
        "X_testN3_GloVe_scaled = MaxAbs_scaler.transform(X_testN3_GloVe)\n",
        "\n",
        "#prior to this block: 8.0 gb\n",
        "#after: 10.6 (temp 14.2)"
      ],
      "metadata": {
        "id": "8yF28cDwt1Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shift +1 for all non-zero values to be usable for Naive Bayes classifier\n",
        "#from scipy.sparse.csc import csc_matrix\n",
        "X_train_GloVe_scaled += 1\n",
        "X_testN0_GloVe_scaled += 1\n",
        "X_testN1_GloVe_scaled += 1\n",
        "X_testN2_GloVe_scaled += 1\n",
        "X_testN3_GloVe_scaled += 1"
      ],
      "metadata": {
        "id": "AA_cISjCnOcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 5: word2vec"
      ],
      "metadata": {
        "id": "sg6tCmf5weJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_INDEX\n",
        "#Creating the.... Embedding Index (aka embedding dictionary)\n",
        "\n",
        "embedding_dim = 50 #based on ...file..  ('...50d.txt')\n",
        "embeddings_index = {} # == 'embeddings_dictionary' in https://www.kaggle.com/code/andreshg/nlp-glove-bert-tf-idf-lstm-explained#5.-Vectorization\n",
        "w2vfile = open('/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v_PCA_reduced-vectors.txt')\n",
        "for line in w2vfile:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "w2vfile.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2TWhuTtwh5Q",
        "outputId": "ea5621cd-a054-415d-c362-bf7c62289ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4027169 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_MATRIX\n",
        "#this is where we need to grab each word from the padded sequences and match them with the embeddings index to create the embedding_matrix\n",
        "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
        "\n",
        "for word, embedding in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[embedding] = embedding_vector\n",
        "        \n",
        "#embedding_matrix #numpy array"
      ],
      "metadata": {
        "id": "G_rBOtE0G5zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking the padded sequences (word indexes per row), and replacing each index with an array of 50 vectors \n",
        "#(taken from the embedding matrix because that is where the indexes match between train and glove)\n",
        "\"\"\"\n",
        "listylist = []\n",
        "for x in X_train_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist.append(y)\n",
        "\n",
        "# the test sets:\n",
        "listylist_testN0 = []\n",
        "for x in X_testN0_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN0.append(y)\n",
        "\n",
        "listylist_testN1 = []\n",
        "for x in X_testN1_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN1.append(y)\n",
        "\n",
        "listylist_testN2 = []\n",
        "for x in X_testN2_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN2.append(y)\n",
        "\n",
        "listylist_testN3 = []\n",
        "for x in X_testN3_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN3.append(y)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsBiCb8GHyo_",
        "outputId": "1fda12fa-b946-407f-bffe-8798810a719b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlistylist = []\\nfor x in X_train_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist.append(y)\\n\\n# the test sets:\\nlistylist_testN0 = []\\nfor x in X_testN0_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN0.append(y)\\n\\nlistylist_testN1 = []\\nfor x in X_testN1_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN1.append(y)\\n\\nlistylist_testN2 = []\\nfor x in X_testN2_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN2.append(y)\\n\\nlistylist_testN3 = []\\nfor x in X_testN3_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN3.append(y)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nested list to np array (np array is better for use later on)\n",
        "\"\"\"\n",
        "nparraylist_train = np.array(listylist)\n",
        "# ---\n",
        "nparraylist_testN0 = np.array(listylist_testN0)\n",
        "nparraylist_testN1 = np.array(listylist_testN1)\n",
        "nparraylist_testN2 = np.array(listylist_testN2)\n",
        "nparraylist_testN3 = np.array(listylist_testN3)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdbdX6CuH-9c",
        "outputId": "9fcd2a1c-a101-4514-9e52-b2423858123b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnparraylist_train = np.array(listylist)\\n# ---\\nnparraylist_testN0 = np.array(listylist_testN0)\\nnparraylist_testN1 = np.array(listylist_testN1)\\nnparraylist_testN2 = np.array(listylist_testN2)\\nnparraylist_testN3 = np.array(listylist_testN3)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting lists to free up ram. Somewhow overwriting with zero work better than delete..\n",
        "\"\"\"\n",
        "listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3  = [], [], [], [], []\n",
        "del listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3\n",
        "\n",
        "gc.collect()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfRuoCVcLoc3",
        "outputId": "5692f501-2d45-4636-afa6-d5b7d60ac9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlistylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3  = [], [], [], [], []\\ndel listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3\\n\\ngc.collect()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape parameters (3dim array to 2dim array)\n",
        "\"\"\"\n",
        "lenDim1_train = len(nparraylist_train) #... #5000\n",
        "lenDim1_testN0 = len(nparraylist_testN0) #21999\n",
        "\n",
        "lenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #9000\n",
        "lenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #9000\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RholNlIMILZy",
        "outputId": "894993dd-f766-4c77-85ca-7bbc6c06c4ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlenDim1_train = len(nparraylist_train) #... #5000\\nlenDim1_testN0 = len(nparraylist_testN0) #21999\\n\\nlenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #9000\\nlenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #9000\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#however these are 3 dims, and need to be 2.. (1 rows, 2nd the array of vectors..)\n",
        "# this can be done with reshape (see below)\n",
        "\"\"\"\n",
        "X_train_w2v = np.reshape(\n",
        "               nparraylist_train,     # the array to be reshaped\n",
        "               (lenDim1_train, lenDim2_train)  # dimensions of the new array\n",
        "              )\n",
        "\n",
        "# --- (change to for loop to get these 4 sets reshaped in one block)\n",
        "X_testN0_w2v = np.reshape(\n",
        "               nparraylist_testN0,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # dims of test\n",
        "              )\n",
        "\n",
        "X_testN1_w2v = np.reshape(\n",
        "               nparraylist_testN1,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\n",
        "              )\n",
        "\n",
        "X_testN2_w2v = np.reshape(\n",
        "               nparraylist_testN2,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  \n",
        "              )\n",
        "\n",
        "X_testN3_w2v = np.reshape(\n",
        "               nparraylist_testN3,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0) \n",
        "              )\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4SVk4NEIpPf",
        "outputId": "c9776e5f-4219-4799-8731-931308888235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_train_w2v = np.reshape(\\n               nparraylist_train,     # the array to be reshaped\\n               (lenDim1_train, lenDim2_train)  # dimensions of the new array\\n              )\\n\\n# --- (change to for loop to get these 4 sets reshaped in one block)\\nX_testN0_w2v = np.reshape(\\n               nparraylist_testN0,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  # dims of test\\n              )\\n\\nX_testN1_w2v = np.reshape(\\n               nparraylist_testN1,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\\n              )\\n\\nX_testN2_w2v = np.reshape(\\n               nparraylist_testN2,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  \\n              )\\n\\nX_testN3_w2v = np.reshape(\\n               nparraylist_testN3,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0) \\n              )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3 = [], [], [], [], []\n",
        "del nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7T_nfdTMPwh",
        "outputId": "83812b25-b579-4ded-b9e3-d8f66027d9c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3 = [], [], [], [], []\\ndel nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the above 5 blocks:\n",
        "# For GloVe this block took about 10 GB\n",
        "\"\"\"\n",
        "def embedding_generator(X, embedding_matrix):\n",
        "    for x in X:\n",
        "        yield embedding_matrix[x]\n",
        "\"\"\"\n",
        "# training set\n",
        "nparraylist_train = np.array(list(embedding_generator(X_train_SeqPad, embedding_matrix)))\n",
        "\n",
        "# test sets\n",
        "test_sets = [X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad]\n",
        "nparraylist_tests = []\n",
        "for X_test in test_sets:\n",
        "    nparraylist_tests.append(np.array(list(embedding_generator(X_test, embedding_matrix))))\n",
        "\n",
        "# reshape arrays\n",
        "dims = (len(nparraylist_train), len(nparraylist_train[0]) * len(nparraylist_train[0][0]))\n",
        "X_train_w2v = np.reshape(nparraylist_train, dims)\n",
        "\n",
        "X_tests_w2v = []\n",
        "for nparraylist_test in nparraylist_tests:\n",
        "    dims = (len(nparraylist_test), len(nparraylist_test[0]) * len(nparraylist_test[0][0]))\n",
        "    X_tests_w2v.append(np.reshape(nparraylist_test, dims))\n",
        "\n",
        "X_testN0_w2v, X_testN1_w2v, X_testN2_w2v, X_testN3_w2v = X_tests_w2v"
      ],
      "metadata": {
        "id": "YE1H8ehnkgse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.array_equal(X_testN0_w2v, X_testN1_w2v, equal_nan=False) #False, so that seems correct\n",
        "#np.array_equal(X_testN0_GloVe, X_testN0_w2v, equal_nan=False) #False, so that seems correct"
      ],
      "metadata": {
        "id": "IVaUEXHPoa5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS BLOCK IS ONLY FOR THE NAIVE BAYES CLASSIFIER\n",
        "\n",
        "#scaling\n",
        "#minmax would be the go to for sparse data, however our sparse data contains negative values -> MaxAbsScaler\n",
        "#from sklearn.preprocessing import MaxAbsScaler\n",
        "#THis takes another 10GB...\n",
        "\n",
        "MaxAbs_scaler = MaxAbsScaler()\n",
        "\n",
        "# transform data\n",
        "# (check whether indeed best to fit scaler only on trainset)\n",
        "MaxAbs_scaler.fit(X_train_w2v)\n",
        "X_train_w2v_scaled = MaxAbs_scaler.transform(X_train_w2v)\n",
        "X_testN0_w2v_scaled = MaxAbs_scaler.transform(X_testN0_w2v)\n",
        "X_testN1_w2v_scaled = MaxAbs_scaler.transform(X_testN1_w2v)\n",
        "X_testN2_w2v_scaled = MaxAbs_scaler.transform(X_testN2_w2v)\n",
        "X_testN3_w2v_scaled = MaxAbs_scaler.transform(X_testN3_w2v)"
      ],
      "metadata": {
        "id": "rWH8--uV4wBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shift +1 for all non-zero values to be usable for Naive Bayes classifier\n",
        "#from scipy.sparse.csc import csc_matrix\n",
        "X_train_w2v_scaled += 1\n",
        "X_testN0_w2v_scaled += 1\n",
        "X_testN1_w2v_scaled += 1\n",
        "X_testN2_w2v_scaled += 1\n",
        "X_testN3_w2v_scaled += 1"
      ],
      "metadata": {
        "id": "cSNLj7qJ5KPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding 6: FastText"
      ],
      "metadata": {
        "id": "gL18k159KePF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_INDEX\n",
        "#Creating the.... Embedding Index (aka embedding dictionary)\n",
        "\n",
        "embedding_dim = 50 #based on ...file..  ('...50d.txt')\n",
        "embeddings_index = {} \n",
        "w2vfile = open('/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/ft_PCA_reduced-vectors.txt')\n",
        "for line in w2vfile:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "w2vfile.close()\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')\n",
        "\n",
        "#note to self: the dim number (50) does not influence the total word vector count (i.e. 400k for either 50 or 100 dims..)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zhJHZymKoIN",
        "outputId": "7b46c9f4-6eea-4a94-bee4-900875c8308c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 999994 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDING_MATRIX\n",
        "#this is where we need to grab each word from the padded sequences and match them with the embeddings index to create the embedding_matrix\n",
        "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
        "\n",
        "for word, embedding in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[embedding] = embedding_vector\n",
        "        \n",
        "#embedding_matrix #numpy array"
      ],
      "metadata": {
        "id": "IQVxAnEHKqXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#taking the padded sequences (word indexes per row), and replacing each index with an array of 50 vectors \n",
        "#(taken from the embedding matrix because that is where the indexes match between train and glove)\n",
        "\"\"\"\n",
        "listylist = []\n",
        "for x in X_train_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist.append(y)\n",
        "\n",
        "# the test sets:\n",
        "listylist_testN0 = []\n",
        "for x in X_testN0_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN0.append(y)\n",
        "\n",
        "listylist_testN1 = []\n",
        "for x in X_testN1_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN1.append(y)\n",
        "\n",
        "listylist_testN2 = []\n",
        "for x in X_testN2_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN2.append(y)\n",
        "\n",
        "listylist_testN3 = []\n",
        "for x in X_testN3_SeqPad:\n",
        "  y = embedding_matrix[x]\n",
        "  listylist_testN3.append(y)\n",
        "  \"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuZUmUsAKvTK",
        "outputId": "4aa9a341-ce09-4323-ba88-e514fcab9dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlistylist = []\\nfor x in X_train_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist.append(y)\\n\\n# the test sets:\\nlistylist_testN0 = []\\nfor x in X_testN0_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN0.append(y)\\n\\nlistylist_testN1 = []\\nfor x in X_testN1_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN1.append(y)\\n\\nlistylist_testN2 = []\\nfor x in X_testN2_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN2.append(y)\\n\\nlistylist_testN3 = []\\nfor x in X_testN3_SeqPad:\\n  y = embedding_matrix[x]\\n  listylist_testN3.append(y)\\n  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nested list to np array (np array is better for use later on)\n",
        "\"\"\"\n",
        "nparraylist_train = np.array(listylist)\n",
        "# ---\n",
        "nparraylist_testN0 = np.array(listylist_testN0)\n",
        "nparraylist_testN1 = np.array(listylist_testN1)\n",
        "nparraylist_testN2 = np.array(listylist_testN2)\n",
        "nparraylist_testN3 = np.array(listylist_testN3)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGZRfr7fKylJ",
        "outputId": "ffc350a3-192f-4255-9b86-88f5c321e33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnparraylist_train = np.array(listylist)\\n# ---\\nnparraylist_testN0 = np.array(listylist_testN0)\\nnparraylist_testN1 = np.array(listylist_testN1)\\nnparraylist_testN2 = np.array(listylist_testN2)\\nnparraylist_testN3 = np.array(listylist_testN3)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deleting lists to free up ram. Somewhow overwriting with zero work better than delete..\n",
        "\"\"\"\n",
        "listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3  = [], [], [], [], []\n",
        "del listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3\n",
        "\n",
        "gc.collect()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr5Y5PY5Lvgg",
        "outputId": "c391ed08-2846-4fcf-b5e2-38aec9c265e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlistylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3  = [], [], [], [], []\\ndel listylist, listylist_testN0, listylist_testN1, listylist_testN2, listylist_testN3\\n\\ngc.collect()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reshape parameters (3dim array to 2dim array)\n",
        "\"\"\"\n",
        "lenDim1_train = len(nparraylist_train) #... #5000\n",
        "lenDim1_testN0 = len(nparraylist_testN0) #21999\n",
        "\n",
        "lenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #9000\n",
        "lenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #9000\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml3MpHwEK1yS",
        "outputId": "b59d6493-56cb-478e-e555-fe19c25ccf09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nlenDim1_train = len(nparraylist_train) #... #5000\\nlenDim1_testN0 = len(nparraylist_testN0) #21999\\n\\nlenDim2_train = len(nparraylist_train[0]) * len(nparraylist_train[0][0]) #9000\\nlenDim2_testN0 = len(nparraylist_testN0[0]) * len(nparraylist_testN0[0][0]) #9000\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#however these are 3 dims, and need to be 2.. (1 rows, 2nd the array of vectors..)\n",
        "# this can be done with reshape (see below)\n",
        "\"\"\"\n",
        "X_train_ft = np.reshape(\n",
        "               nparraylist_train,     # the array to be reshaped\n",
        "               (lenDim1_train, lenDim2_train)  # dimensions of the new array\n",
        "              )\n",
        "\n",
        "# --- (change to for loop to get these 4 sets reshaped in one block)\n",
        "X_testN0_ft = np.reshape(\n",
        "               nparraylist_testN0,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # dims of test\n",
        "              )\n",
        "\n",
        "X_testN1_ft = np.reshape(\n",
        "               nparraylist_testN1,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\n",
        "              )\n",
        "\n",
        "X_testN2_ft = np.reshape(\n",
        "               nparraylist_testN2,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0)  \n",
        "              )\n",
        "\n",
        "X_testN3_ft = np.reshape(\n",
        "               nparraylist_testN3,     # the array to be reshaped\n",
        "               (lenDim1_testN0, lenDim2_testN0) \n",
        "              )\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFy1-wtvK4qN",
        "outputId": "baef03de-53ef-42c4-e5f9-d0194efba946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nX_train_ft = np.reshape(\\n               nparraylist_train,     # the array to be reshaped\\n               (lenDim1_train, lenDim2_train)  # dimensions of the new array\\n              )\\n\\n# --- (change to for loop to get these 4 sets reshaped in one block)\\nX_testN0_ft = np.reshape(\\n               nparraylist_testN0,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  # dims of test\\n              )\\n\\nX_testN1_ft = np.reshape(\\n               nparraylist_testN1,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  # no diff between N0 and N1 ..\\n              )\\n\\nX_testN2_ft = np.reshape(\\n               nparraylist_testN2,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0)  \\n              )\\n\\nX_testN3_ft = np.reshape(\\n               nparraylist_testN3,     # the array to be reshaped\\n               (lenDim1_testN0, lenDim2_testN0) \\n              )\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the above 5 blocks:\n",
        "# For GloVe this block took about 10 GB\n",
        "\"\"\"\n",
        "def embedding_generator(X, embedding_matrix):\n",
        "    for x in X:\n",
        "        yield embedding_matrix[x]\n",
        "\"\"\"\n",
        "# training set\n",
        "nparraylist_train = np.array(list(embedding_generator(X_train_SeqPad, embedding_matrix)))\n",
        "\n",
        "# test sets\n",
        "test_sets = [X_testN0_SeqPad, X_testN1_SeqPad, X_testN2_SeqPad, X_testN3_SeqPad]\n",
        "nparraylist_tests = []\n",
        "for X_test in test_sets:\n",
        "    nparraylist_tests.append(np.array(list(embedding_generator(X_test, embedding_matrix))))\n",
        "\n",
        "# reshape arrays\n",
        "dims = (len(nparraylist_train), len(nparraylist_train[0]) * len(nparraylist_train[0][0]))\n",
        "X_train_ft = np.reshape(nparraylist_train, dims)\n",
        "\n",
        "X_tests_ft = []\n",
        "for nparraylist_test in nparraylist_tests:\n",
        "    dims = (len(nparraylist_test), len(nparraylist_test[0]) * len(nparraylist_test[0][0]))\n",
        "    X_tests_ft.append(np.reshape(nparraylist_test, dims))\n",
        "\n",
        "X_testN0_ft, X_testN1_ft, X_testN2_ft, X_testN3_ft = X_tests_ft"
      ],
      "metadata": {
        "id": "iWTjE3di54Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS BLOCK IS ONLY FOR THE NAIVE BAYES CLASSIFIER\n",
        "\n",
        "#scaling\n",
        "#minmax would be the go to for sparse data, however our sparse data contains negative values -> MaxAbsScaler\n",
        "#from sklearn.preprocessing import MaxAbsScaler\n",
        "#THis takes another 10GB...\n",
        "\n",
        "MaxAbs_scaler = MaxAbsScaler()\n",
        "\n",
        "# transform data\n",
        "# (check whether indeed best to fit scaler only on trainset)\n",
        "MaxAbs_scaler.fit(X_train_ft)\n",
        "X_train_ft_scaled = MaxAbs_scaler.transform(X_train_ft)\n",
        "X_testN0_ft_scaled = MaxAbs_scaler.transform(X_testN0_ft)\n",
        "X_testN1_ft_scaled = MaxAbs_scaler.transform(X_testN1_ft)\n",
        "X_testN2_ft_scaled = MaxAbs_scaler.transform(X_testN2_ft)\n",
        "X_testN3_ft_scaled = MaxAbs_scaler.transform(X_testN3_ft)"
      ],
      "metadata": {
        "id": "egyd16ul5aRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shift +1 for all non-zero values to be usable for Naive Bayes classifier\n",
        "#from scipy.sparse.csc import csc_matrix\n",
        "X_train_ft_scaled += 1\n",
        "X_testN0_ft_scaled += 1\n",
        "X_testN1_ft_scaled += 1\n",
        "X_testN2_ft_scaled += 1\n",
        "X_testN3_ft_scaled += 1"
      ],
      "metadata": {
        "id": "IJzlal3u5b18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nparraylist_train, nparraylist_test, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3 = [], [], [], [], [], []\n",
        "del nparraylist_train, nparraylist_testN0, nparraylist_testN1, nparraylist_testN2, nparraylist_testN3"
      ],
      "metadata": {
        "id": "ZhECAOD7MUkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3AOa6Ob-xA1",
        "outputId": "3b41823f-48b6-437c-9e3f-ab8eaeaa88b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check RAM"
      ],
      "metadata": {
        "id": "JdSQhnhUNJYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# also see del statement under glove embedding code\n",
        "# and the gc collect"
      ],
      "metadata": {
        "id": "5IsmVZT4NYgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code snippet to list all self defined vars and their RAM uptake\n",
        "#source: https://stackoverflow.com/questions/40993626/list-memory-usage-in-ipython-and-jupyter\n",
        "\n",
        "#import sys\n",
        "\n",
        "# These are the usual ipython objects, including this one you are creating\n",
        "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
        "\n",
        "# Get a sorted list of the objects and their sizes\n",
        "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "xUdK74wPNPtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c13087-448b-494f-df7c-955ac17bb571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('X_testN0_GloVe_scaled', 703968120),\n",
              " ('X_testN0_ft_scaled', 703968120),\n",
              " ('X_testN0_w2v_scaled', 703968120),\n",
              " ('X_testN1_GloVe_scaled', 703968120),\n",
              " ('X_testN1_ft_scaled', 703968120),\n",
              " ('X_testN1_w2v_scaled', 703968120),\n",
              " ('X_testN2_GloVe_scaled', 703968120),\n",
              " ('X_testN2_ft_scaled', 703968120),\n",
              " ('X_testN2_w2v_scaled', 703968120),\n",
              " ('X_testN3_GloVe_scaled', 703968120),\n",
              " ('X_testN3_ft_scaled', 703968120),\n",
              " ('X_testN3_w2v_scaled', 703968120),\n",
              " ('X_train_GloVe_scaled', 640000120),\n",
              " ('X_train_ft_scaled', 640000120),\n",
              " ('X_train_w2v_scaled', 640000120),\n",
              " ('df_test', 238116585),\n",
              " ('X_testN0', 59800700),\n",
              " ('X_testN1', 59680177),\n",
              " ('X_testN2', 59301074),\n",
              " ('X_testN3', 59159074),\n",
              " ('df_train', 54681740),\n",
              " ('X_train', 54521740),\n",
              " ('embedding_matrix', 45494920),\n",
              " ('embeddings_index', 41943136),\n",
              " ('X_test', 7039800),\n",
              " ('X_testN0_SeqPad', 7039800),\n",
              " ('X_testN1_SeqPad', 7039800),\n",
              " ('X_testN2_SeqPad', 7039800),\n",
              " ('X_testN3_SeqPad', 7039800),\n",
              " ('X_train_SeqPad', 6400120),\n",
              " ('word_index', 5242968),\n",
              " ('y_train', 320016),\n",
              " ('X_testN0_sequences', 190000),\n",
              " ('X_testN1_sequences', 190000),\n",
              " ('X_testN2_sequences', 190000),\n",
              " ('X_testN3_sequences', 190000),\n",
              " ('y_testALL', 176136),\n",
              " ('X_train_sequences', 168848),\n",
              " ('CountVectorizer', 1472),\n",
              " ('HashingVectorizer', 1472),\n",
              " ('Tokenizer', 1472),\n",
              " ('TfidfTransformer', 1192),\n",
              " ('csc_matrix', 1192),\n",
              " ('GradientBoostingClassifier', 1064),\n",
              " ('KNeighborsClassifier', 1064),\n",
              " ('LogisticRegression', 1064),\n",
              " ('MaxAbsScaler', 1064),\n",
              " ('MultinomialNB', 1064),\n",
              " ('RandomForestClassifier', 1064),\n",
              " ('SGDClassifier', 1064),\n",
              " ('XGBClassifier', 1064),\n",
              " ('line', 652),\n",
              " ('values', 464),\n",
              " ('coefs', 304),\n",
              " ('glovefile', 208),\n",
              " ('w2vfile', 208),\n",
              " ('embedding_generator', 136),\n",
              " ('pad_sequences', 136),\n",
              " ('roc_auc_score', 136),\n",
              " ('X_testN0_GloVe', 120),\n",
              " ('X_testN0_ft', 120),\n",
              " ('X_testN0_w2v', 120),\n",
              " ('X_testN1_GloVe', 120),\n",
              " ('X_testN1_ft', 120),\n",
              " ('X_testN1_w2v', 120),\n",
              " ('X_testN2_GloVe', 120),\n",
              " ('X_testN2_ft', 120),\n",
              " ('X_testN2_w2v', 120),\n",
              " ('X_testN3_GloVe', 120),\n",
              " ('X_testN3_ft', 120),\n",
              " ('X_testN3_w2v', 120),\n",
              " ('X_train_GloVe', 120),\n",
              " ('X_train_ft', 120),\n",
              " ('X_train_w2v', 120),\n",
              " ('pathTRAINcsv', 104),\n",
              " ('pathTESTcsv', 97),\n",
              " ('picklepath', 93),\n",
              " ('word', 90),\n",
              " ('X_tests_GloVe', 88),\n",
              " ('X_tests_ft', 88),\n",
              " ('X_tests_w2v', 88),\n",
              " ('nparraylist_tests', 88),\n",
              " ('test_sets', 88),\n",
              " ('drive', 72),\n",
              " ('dump', 72),\n",
              " ('load', 72),\n",
              " ('np', 72),\n",
              " ('pd', 72),\n",
              " ('plt', 72),\n",
              " ('dims', 56),\n",
              " ('nparraylist_test', 56),\n",
              " ('padding_type', 53),\n",
              " ('truncation_type', 53),\n",
              " ('MaxAbs_scaler', 48),\n",
              " ('X_testN0_BoW', 48),\n",
              " ('X_testN0_HV', 48),\n",
              " ('X_testN0_HV_scaled', 48),\n",
              " ('X_testN0_TFIDF', 48),\n",
              " ('X_testN1_BoW', 48),\n",
              " ('X_testN1_HV', 48),\n",
              " ('X_testN1_HV_scaled', 48),\n",
              " ('X_testN1_TFIDF', 48),\n",
              " ('X_testN2_BoW', 48),\n",
              " ('X_testN2_HV', 48),\n",
              " ('X_testN2_HV_scaled', 48),\n",
              " ('X_testN2_TFIDF', 48),\n",
              " ('X_testN3_BoW', 48),\n",
              " ('X_testN3_HV', 48),\n",
              " ('X_testN3_HV_scaled', 48),\n",
              " ('X_testN3_TFIDF', 48),\n",
              " ('X_train_BoW', 48),\n",
              " ('X_train_HV', 48),\n",
              " ('X_train_HV_scaled', 48),\n",
              " ('X_train_TFIDF', 48),\n",
              " ('hashingvectorizer', 48),\n",
              " ('tfidf_transformer', 48),\n",
              " ('vectorizer', 48),\n",
              " ('word_tokenizer', 48),\n",
              " ('embedding', 28),\n",
              " ('embedding_dim', 28),\n",
              " ('max_len', 28),\n",
              " ('maxlenCalc', 28),\n",
              " ('vocab_length', 28),\n",
              " ('embedding_vector', 16)]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 0: Creating lists for iterative model comparisons."
      ],
      "metadata": {
        "id": "F0nj56E2Eb1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ListTrainsetEmbeds = [X_train_BoW, X_train_HV, X_train_TFIDF, X_train_GloVe]\n",
        "#ListModelNames_LR = ['Model_LR_BoW', 'Model_LR_HV', 'Model_LR_TFIDF', 'Model_LR_GloVe']\n",
        "\n",
        "ListTestsets_BoW = [X_testN0_BoW, X_testN1_BoW, X_testN2_BoW, X_testN3_BoW]\n",
        "ListTestsets_HV = [X_testN0_HV, X_testN1_HV, X_testN2_HV, X_testN3_HV]\n",
        "ListTestsets_TFIDF = [X_testN0_TFIDF, X_testN1_TFIDF, X_testN2_TFIDF, X_testN3_TFIDF]\n",
        "ListTestsets_GloVe = [X_testN0_GloVe, X_testN1_GloVe, X_testN2_GloVe, X_testN3_GloVe]\n",
        "ListTestsets_w2v = [X_testN0_w2v, X_testN1_w2v, X_testN2_w2v, X_testN3_w2v]\n",
        "ListTestsets_ft = [X_testN0_ft, X_testN1_ft, X_testN2_ft, X_testN3_ft]\n",
        "\n",
        "ListTestsets_HV_scaled = [X_testN0_HV_scaled, X_testN1_HV_scaled, X_testN2_HV_scaled, X_testN3_HV_scaled]\n",
        "ListTestsets_GloVe_scaled = [X_testN0_GloVe_scaled, X_testN1_GloVe_scaled, X_testN2_GloVe_scaled, X_testN3_GloVe_scaled]\n",
        "ListTestsets_w2v_scaled = [X_testN0_w2v_scaled, X_testN1_w2v_scaled, X_testN2_w2v_scaled, X_testN3_w2v_scaled]\n",
        "ListTestsets_ft_scaled = [X_testN0_ft_scaled, X_testN1_ft_scaled, X_testN2_ft_scaled, X_testN3_ft_scaled]\n",
        "\n",
        "names_list = ['X_testN0', 'X_testN1', 'X_testN2', 'X_testN3']"
      ],
      "metadata": {
        "id": "DJ4mojmhomm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: Logistics Regression"
      ],
      "metadata": {
        "id": "ythDSEFqMIfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.linear_model import LogisticRegression\n",
        "classifierLR = LogisticRegression(solver='lbfgs', max_iter=2000)"
      ],
      "metadata": {
        "id": "LaH7T6O4xgq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\n",
        "Model_LR_BoW = classifierLR.fit(X_train_BoW, y_train)\n",
        "with open(picklepath+\"Model_LR_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_LR_BoW, f)\n",
        "\n",
        "Model_LR_HV = classifierLR.fit(X_train_HV, y_train)\n",
        "with open(picklepath+\"Model_LR_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_LR_HV, f)\n",
        "\n",
        "Model_LR_TFIDF = classifierLR.fit(X_train_TFIDF, y_train)\n",
        "with open(picklepath+\"Model_LR_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_LR_TFIDF, f)\n",
        "\n",
        "Model_LR_GloVe = classifierLR.fit(X_train_GloVe, y_train)\n",
        "with open(picklepath+\"Model_LR_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_LR_GloVe, f)"
      ],
      "metadata": {
        "id": "QvOn5g3R2nrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_LR_w2v= classifierLR.fit(X_train_w2v, y_train)\n",
        "with open(picklepath+\"Model_LR_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_LR_w2v, f)"
      ],
      "metadata": {
        "id": "NHaKi6beJVLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_LR_ft= classifierLR.fit(X_train_ft, y_train)\n",
        "with open(picklepath+\"Model_LR_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_LR_ft, f)"
      ],
      "metadata": {
        "id": "T_2IPcK7rBOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking datatset shapes:\n",
        "# --- \n",
        "#X_train.shape # (67625,)\n",
        "#X_train_xxxx -> are all (67625, 60326) except X_train_GloVe with 67625 by 5000\n",
        "#X_train_BoW.shape == X_train_TFIDF.shape == X_train_HV.shape\n",
        "#X_train_GloVe.shape #(67625, 5000)"
      ],
      "metadata": {
        "id": "VGAJsMDV-lFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no longer all the same shape... :\n",
        "#X_train_BoW.shape # (67625, 53921)\n",
        "#X_train_TFIDF.shape # (67625, 53921)\n",
        "#X_train_HV.shape # (67625, 60326)"
      ],
      "metadata": {
        "id": "NoCpmTikW66d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Logistic regression with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "DDi2daJ_MQl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_LR_BoW.pkl', 'rb') as f:\n",
        "  Model_LR_BoW = load(f)\n",
        "  score_test = Model_LR_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + LR): {score_test}') #.9966..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_LR_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)    \n",
        "    #score_test = Model_LR_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + LogisticRegression on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkNMzo4jUEs0",
        "outputId": "dfadb6f1-82d7-46d1-b492-65953c88a089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (BoW + LR): 0.9966\n",
            "AUC score for BoW + LogisticRegression on X_testN0 (1%split): 0.8866675492424502\n",
            "AUC score for BoW + LogisticRegression on X_testN1 (1%split): 0.8466145617148602\n",
            "AUC score for BoW + LogisticRegression on X_testN2 (1%split): 0.843473480522954\n",
            "AUC score for BoW + LogisticRegression on X_testN3 (1%split): 0.843053156292034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Logistic regression with HashingVectorizer.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "GHJ4hptlwosd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath+ 'Model_LR_HV.pkl', 'rb') as f:\n",
        "  Model_LR_HV = load(f)\n",
        "  score_test = Model_LR_HV.score(X_train_HV, y_train)\n",
        "  print(f'Model overfit check (HV + LR): {score_test}') #.9216..\n",
        "  for i, name in zip(ListTestsets_HV, names_list):\n",
        "    y_pred = Model_LR_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred) \n",
        "    #score_test = Model_LR_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HV + LogisticRegression on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRH-FbWXwksS",
        "outputId": "b2fad6fa-cdc4-46f0-84c0-f377c5631472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (HV + LR): 0.8978\n",
            "AUC score for HV + LogisticRegression on X_testN0 (1%split): 0.8700282998234345\n",
            "AUC score for HV + LogisticRegression on X_testN1 (1%split): 0.8386208978311523\n",
            "AUC score for HV + LogisticRegression on X_testN2 (1%split): 0.8366524668421289\n",
            "AUC score for HV + LogisticRegression on X_testN3 (1%split): 0.8380999745452902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Logistic regression with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "j2gMrqsziAjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_LR_TFIDF.pkl', 'rb') as f:\n",
        "  Model_LR_TFIDF = load(f)\n",
        "  score_test = Model_LR_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + LR): {score_test}') #.9510..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_LR_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_LR_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + LogisticRegression on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZ0hPzFoghze",
        "outputId": "ae27e179-f271-4faa-ea0a-8bd5b3f4ebf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (TFIDF + LR): 0.9404\n",
            "AUC score for TFIDF + LogisticRegression on X_testN0 (1%split): 0.893603485328009\n",
            "AUC score for TFIDF + LogisticRegression on X_testN1 (1%split): 0.8624777606895653\n",
            "AUC score for TFIDF + LogisticRegression on X_testN2 (1%split): 0.8600242127175137\n",
            "AUC score for TFIDF + LogisticRegression on X_testN3 (1%split): 0.8606497789083027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Logistic regression with GloVe.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "pj0-LyZcyqt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_LR_GloVe.pkl', 'rb') as f:\n",
        "  Model_LR_GloVe = load(f)\n",
        "  score_test = Model_LR_GloVe.score(X_train_GloVe, y_train)\n",
        "  print(f'Model overfit check (GloVe + LR): {score_test}') #.8634..\n",
        "  for i, name in zip(ListTestsets_GloVe, names_list):\n",
        "    y_pred = Model_LR_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_LR_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe+ LogisticRegression on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajnVz_5zz8AA",
        "outputId": "171d9511-7c8c-4ea9-da27-9ceaaa9a4418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (GloVe + LR): 0.9589\n",
            "AUC score for GloVe+ LogisticRegression on X_testN0 (1%split): 0.7090388284011981\n",
            "AUC score for GloVe+ LogisticRegression on X_testN1 (1%split): 0.6733151593213087\n",
            "AUC score for GloVe+ LogisticRegression on X_testN2 (1%split): 0.6714070877652506\n",
            "AUC score for GloVe+ LogisticRegression on X_testN3 (1%split): 0.6702346322969471\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Logistic regression with Word2Vec.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "1aZLZDztJ8tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_LR_w2v.pkl', 'rb') as f:\n",
        "  Model_LR_w2v = load(f)\n",
        "  score_test = Model_LR_w2v.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (w2v + LR): {score_test}') #.9966..\n",
        "  for i, name in zip(ListTestsets_w2v, names_list):\n",
        "    y_pred = Model_LR_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_LR_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for w2v + LogisticRegression on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VIp7TAtJshz",
        "outputId": "00f74116-e07e-4357-cfc6-a6e0dcaf812c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (w2v + LR): 0.9696\n",
            "AUC score for w2v + LogisticRegression on X_testN0 (1%split): 0.7297898321668015\n",
            "AUC score for w2v + LogisticRegression on X_testN1 (1%split): 0.6918351511626756\n",
            "AUC score for w2v + LogisticRegression on X_testN2 (1%split): 0.6854994491994577\n",
            "AUC score for w2v + LogisticRegression on X_testN3 (1%split): 0.6882848041779732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Logistic regression with FastText.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "Eqw1kb5muevK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_LR_ft.pkl', 'rb') as f:\n",
        "  Model_LR_ft = load(f)\n",
        "  score_test = Model_LR_ft.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (fasttext + LR): {score_test}') #.9966..\n",
        "  for i, name in zip(ListTestsets_ft, names_list):\n",
        "    y_pred = Model_LR_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_LR_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for fasttext + LogisticRegression on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzkkiqtorZet",
        "outputId": "78522817-df22-4ccc-90c3-bac8b27b5386"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (fasttext + LR): 0.5256\n",
            "AUC score for fasttext + LogisticRegression on X_testN0 (1%split): 0.748884286873436\n",
            "AUC score for fasttext + LogisticRegression on X_testN1 (1%split): 0.7199036382037689\n",
            "AUC score for fasttext + LogisticRegression on X_testN2 (1%split): 0.7137208567479569\n",
            "AUC score for fasttext + LogisticRegression on X_testN3 (1%split): 0.7141767925471315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: Naive Bayes"
      ],
      "metadata": {
        "id": "srCAridl0STZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#why MultinomialNB instead of Gaussian? GaussianNN seemed to need dense matrix as input and that kept overloading RAM\n",
        "#as train set is about 60k by 67k which is fine for sparse but giant in dense matrix (see code at the bottom with failed attempts to use .todense function).\n",
        "#from sklearn.naive_bayes import MultinomialNB\n",
        "classifierNB = MultinomialNB()"
      ],
      "metadata": {
        "id": "-ALuuidtRv52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\n",
        "Model_NB_BoW = classifierNB.fit(X_train_BoW, y_train, sample_weight=None)\n",
        "with open(picklepath+\"Model_NB_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_NB_BoW, f)\n",
        "\n",
        "Model_NB_HV = classifierNB.fit(X_train_HV_scaled, y_train, sample_weight=None)\n",
        "with open(picklepath+\"Model_NB_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_NB_HV, f)\n",
        "\n",
        "Model_NB_TFIDF = classifierNB.fit(X_train_TFIDF, y_train, sample_weight=None)\n",
        "with open(picklepath+\"Model_NB_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_NB_TFIDF, f)\n",
        "\n",
        "Model_NB_GloVe = classifierNB.fit(X_train_GloVe_scaled, y_train, sample_weight=None)\n",
        "with open(picklepath+\"Model_NB_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_NB_GloVe, f)"
      ],
      "metadata": {
        "id": "gBjGuU7OSGcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_NB_w2v = classifierNB.fit(X_train_w2v_scaled, y_train, sample_weight=None)\n",
        "with open(picklepath+\"Model_NB_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_NB_w2v, f)"
      ],
      "metadata": {
        "id": "0Vbl48zCMwVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_NB_ft = classifierNB.fit(X_train_ft_scaled, y_train, sample_weight=None)\n",
        "with open(picklepath+\"Model_NB_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_NB_ft, f)"
      ],
      "metadata": {
        "id": "7d8MU5UDM_Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Naive Bayes with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o-BZnWoiQVNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_NB_BoW.pkl', 'rb') as f:\n",
        "  Model_NB_BoW = load(f)\n",
        "  score_test = Model_NB_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + NB): {score_test}') #.8730..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_NB_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_NB_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + Naive Bayes on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F2mZaVK1PqV",
        "outputId": "a6b7dcd8-045b-4c8f-83fa-392e81ea629b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (BoW + NB): 0.8623\n",
            "AUC score for BoW + Naive Bayes on X_testN0 (1%split): 0.8221171256731954\n",
            "AUC score for BoW + Naive Bayes on X_testN1 (1%split): 0.7904750560831445\n",
            "AUC score for BoW + Naive Bayes on X_testN2 (1%split): 0.7889995853230718\n",
            "AUC score for BoW + Naive Bayes on X_testN3 (1%split): 0.7881931355679659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Naive Bayes with HV. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)\n"
      ],
      "metadata": {
        "id": "hYNNV4l-r4oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this block uses the scaled input (MaxAbs scaler with a +1 shift for all nonzero values..)\n",
        "with open(picklepath + 'Model_NB_HV.pkl', 'rb') as f:\n",
        "  Model_NB_HV = load(f)\n",
        "  score_test = Model_NB_HV.score(X_train_HV_scaled, y_train)\n",
        "  print(f'Model overfit check (HV + NB): {score_test}') #.8682..\n",
        "  for i, name in zip(ListTestsets_HV_scaled, names_list):\n",
        "    y_pred = Model_NB_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_NB_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HashingVectors + Naive Bayes on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZurTQEwr_Ow",
        "outputId": "c89de856-4921-478b-86c4-2b3900770996"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (HV + NB): 0.8887\n",
            "AUC score for HashingVectors + Naive Bayes on X_testN0 (1%split): 0.8121159098664951\n",
            "AUC score for HashingVectors + Naive Bayes on X_testN1 (1%split): 0.7784890888723706\n",
            "AUC score for HashingVectors + Naive Bayes on X_testN2 (1%split): 0.7758224556945271\n",
            "AUC score for HashingVectors + Naive Bayes on X_testN3 (1%split): 0.7752940786319006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Naive Bayes with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "LX18EFQ0kGY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_NB_TFIDF.pkl', 'rb') as f:\n",
        "  Model_NB_TFIDF = load(f)\n",
        "  score_test = Model_NB_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + NB): {score_test}') #.8753..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_NB_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_NB_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + Naive Bayes on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdWdvFvjeky",
        "outputId": "199dc228-bf76-4fb6-9233-a234cbf43506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (TFIDF + NB): 0.8797\n",
            "AUC score for TFIDF + Naive Bayes on X_testN0 (1%split): 0.8365438256633956\n",
            "AUC score for TFIDF + Naive Bayes on X_testN1 (1%split): 0.803262501102322\n",
            "AUC score for TFIDF + Naive Bayes on X_testN2 (1%split): 0.8028437844675026\n",
            "AUC score for TFIDF + Naive Bayes on X_testN3 (1%split): 0.80232466351232\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Naive Bayes with GloVe.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "97uyQAl09Rjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_NB_GloVe.pkl', 'rb') as f:\n",
        "  Model_NB_GloVe = load(f)\n",
        "  score_test = Model_NB_GloVe.score(X_train_GloVe_scaled, y_train)\n",
        "  print(f'Model overfit check (GloVe + NB): {score_test}')\n",
        "  for i, name in zip(ListTestsets_GloVe_scaled, names_list):\n",
        "    y_pred = Model_NB_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_NB_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe + Naive Bayes on {name} (1%split): {score_test}')\n",
        "#Remember this uses the scaled and +1 shifted embeddings.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-UgjxSd9P0S",
        "outputId": "06ff025b-6bae-4ed2-9802-d8a1eb922263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (GloVe + NB): 0.6347\n",
            "AUC score for GloVe + Naive Bayes on X_testN0 (1%split): 0.6270191676334755\n",
            "AUC score for GloVe + Naive Bayes on X_testN1 (1%split): 0.6009188215511814\n",
            "AUC score for GloVe + Naive Bayes on X_testN2 (1%split): 0.5995393217923292\n",
            "AUC score for GloVe + Naive Bayes on X_testN3 (1%split): 0.5995129663315183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Naive Bayes with Word2Vec.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "OWapz94ONhm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_NB_w2v.pkl', 'rb') as f:\n",
        "  Model_NB_w2v = load(f)\n",
        "  score_test = Model_NB_w2v.score(X_train_w2v_scaled, y_train)\n",
        "  print(f'Model overfit check (w2v + NB): {score_test}')\n",
        "  for i, name in zip(ListTestsets_w2v_scaled, names_list):\n",
        "    y_pred = Model_NB_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_NB_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for w2v + Naive Bayes on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnQcJY1uNHC5",
        "outputId": "23d7ec5a-8e57-43e3-b061-bfe9f00a62e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (w2v + NB): 0.6255\n",
            "AUC score for w2v + Naive Bayes on X_testN0 (1%split): 0.6158662500926647\n",
            "AUC score for w2v + Naive Bayes on X_testN1 (1%split): 0.5768944703815251\n",
            "AUC score for w2v + Naive Bayes on X_testN2 (1%split): 0.5767445703317411\n",
            "AUC score for w2v + Naive Bayes on X_testN3 (1%split): 0.5759582403876236\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Naive Bayes with FastText.\n",
        "\n",
        "(N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "E2AAZFVTNk0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_NB_ft.pkl', 'rb') as f:\n",
        "  Model_NB_ft = load(f)\n",
        "  score_test = Model_NB_ft.score(X_train_ft_scaled, y_train)\n",
        "  print(f'Model overfit check (FastText + NB): {score_test}')\n",
        "  for i, name in zip(ListTestsets_ft_scaled, names_list):\n",
        "    y_pred = Model_NB_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_NB_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for FastText + Naive Bayes on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcmPGQmrNVaj",
        "outputId": "472b75c3-a1d2-4151-d98c-d3fac6fb2810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (FastText + NB): 0.573\n",
            "AUC score for FastText + Naive Bayes on X_testN0 (1%split): 0.5614832089321884\n",
            "AUC score for FastText + Naive Bayes on X_testN1 (1%split): 0.5399177671504443\n",
            "AUC score for FastText + Naive Bayes on X_testN2 (1%split): 0.5388207734032141\n",
            "AUC score for FastText + Naive Bayes on X_testN3 (1%split): 0.538307888097854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check some more MN Naive Bayes evaluation code here:\n",
        "https://www.kaggle.com/code/ibrahimaptlo10/eda-multi-class-multinomial-naive-bayes"
      ],
      "metadata": {
        "id": "Evz8GhaWQ4rV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: Random Forest\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "W-88O9XEEtQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "classifierRF = RandomForestClassifier(n_estimators= 20)"
      ],
      "metadata": {
        "id": "v9iqsyu1FO9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#still to do:\n",
        "# add n_estimators’ and ‘max_features’ etc to prevent overfitting.. \n",
        "# Prioritize speed of training over precision for now! (running this now takes about 15 minutes, with n_estimators at 20, this is about 2 minutes)"
      ],
      "metadata": {
        "id": "Ps2EWbwjVB99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\n",
        "Model_RF_BoW = classifierRF.fit(X_train_BoW, y_train)\n",
        "with open(picklepath+\"Model_RF_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_RF_BoW, f)\n",
        "\n",
        "Model_RF_HV = classifierRF.fit(X_train_HV_scaled, y_train)\n",
        "with open(picklepath+\"Model_RF_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_RF_HV, f)\n",
        "# -> Negative values in data passed to MultinomialNB (input X)\n",
        "\n",
        "Model_RF_TFIDF = classifierRF.fit(X_train_TFIDF, y_train)\n",
        "with open(picklepath+\"Model_RF_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_RF_TFIDF, f)\n",
        "\n",
        "Model_RF_GloVe = classifierRF.fit(X_train_GloVe_scaled, y_train)\n",
        "with open(picklepath+\"Model_RF_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_RF_GloVe, f)"
      ],
      "metadata": {
        "id": "H_pD_3JqVV9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_RF_w2v = classifierRF.fit(X_train_w2v, y_train)\n",
        "with open(picklepath+\"Model_RF_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_RF_w2v, f)"
      ],
      "metadata": {
        "id": "fhwRCYmRNrzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_RF_ft = classifierRF.fit(X_train_ft, y_train)\n",
        "with open(picklepath+\"Model_RF_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_RF_ft, f)"
      ],
      "metadata": {
        "id": "08XAKu-nNsHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Random Forest with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "btaIZGCwREdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_RF_BoW.pkl', 'rb') as f:\n",
        "  Model_RF_BoW = load(f)\n",
        "  score_test = Model_RF_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + RF): {score_test}') #.9994..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_RF_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_RF_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + Random Forest on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WFzxGGXGcsL",
        "outputId": "8f09575a-c9d9-4f5d-98ea-3bd99bb750e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (BoW + RF): 0.9991\n",
            "AUC score for BoW + Random Forest on X_testN0 (1%split): 0.8348274516557319\n",
            "AUC score for BoW + Random Forest on X_testN1 (1%split): 0.8052598688211525\n",
            "AUC score for BoW + Random Forest on X_testN2 (1%split): 0.8004738812121387\n",
            "AUC score for BoW + Random Forest on X_testN3 (1%split): 0.8026676739720198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Random Forest with HV. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "k5NHAhRIXMpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_RF_HV.pkl', 'rb') as f:\n",
        "  Model_RF_HV = load(f)\n",
        "  score_test = Model_RF_HV.score(X_train_HV_scaled, y_train)\n",
        "  print(f'Model overfit check (HV + RF): {score_test}') #.9994..\n",
        "  for i, name in zip(ListTestsets_HV_scaled, names_list):\n",
        "    y_pred = Model_RF_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_RF_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HV + Random Forest on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-6soEn0XJaH",
        "outputId": "63626f36-d8cf-4b3c-b693-a11bf580f73b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (HV + RF): 0.9995\n",
            "AUC score for HV + Random Forest on X_testN0 (1%split): 0.8186271836976364\n",
            "AUC score for HV + Random Forest on X_testN1 (1%split): 0.795164923532537\n",
            "AUC score for HV + Random Forest on X_testN2 (1%split): 0.7913751358812319\n",
            "AUC score for HV + Random Forest on X_testN3 (1%split): 0.794681024676451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Random Forest with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "gguAjabKkrMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_RF_TFIDF.pkl', 'rb') as f:\n",
        "  Model_RF_TFIDF = load(f)\n",
        "  score_test = Model_RF_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + RF): {score_test}') #.9995..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_RF_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_RF_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + Random Forest on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sP71r-ikthy",
        "outputId": "5539afa3-f9ba-4e85-f33e-0b60940466ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (TFIDF + RF): 0.9992\n",
            "AUC score for TFIDF + Random Forest on X_testN0 (1%split): 0.835133926593748\n",
            "AUC score for TFIDF + Random Forest on X_testN1 (1%split): 0.8059355646645671\n",
            "AUC score for TFIDF + Random Forest on X_testN2 (1%split): 0.8062381904871776\n",
            "AUC score for TFIDF + Random Forest on X_testN3 (1%split): 0.8062074017072239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Random Forest with GloVe. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "I1Heji4HXxdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_RF_GloVe.pkl', 'rb') as f:\n",
        "  Model_RF_GloVe = load(f)\n",
        "  score_test = Model_RF_GloVe.score(X_train_GloVe_scaled, y_train)\n",
        "  print(f'Model overfit check (GloVe + RF): {score_test}') #.9908..\n",
        "  for i, name in zip(ListTestsets_GloVe_scaled, names_list):\n",
        "    y_pred = Model_RF_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_RF_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe + Random Forest on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dku31W5_XxDs",
        "outputId": "813578a9-de44-445d-99a1-f4494af9da69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (GloVe + RF): 0.9985\n",
            "AUC score for GloVe + Random Forest on X_testN0 (1%split): 0.7067303535473406\n",
            "AUC score for GloVe + Random Forest on X_testN1 (1%split): 0.6671998471822462\n",
            "AUC score for GloVe + Random Forest on X_testN2 (1%split): 0.6616675745165127\n",
            "AUC score for GloVe + Random Forest on X_testN3 (1%split): 0.6648875273562722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Random Forest with Word2Vec. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "mkwhgmlqOPMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_RF_w2v.pkl', 'rb') as f:\n",
        "  Model_RF_w2v = load(f)\n",
        "  score_test = Model_RF_w2v.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (w2v + RF): {score_test}') #.9994..\n",
        "  for i, name in zip(ListTestsets_w2v, names_list):\n",
        "    y_pred = Model_RF_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_RF_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for w2v + Random Forest on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udJ7CYwyN6Em",
        "outputId": "4e47aa8f-44a5-4cdd-de2c-ed58e63f48c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (w2v + RF): 0.9987\n",
            "AUC score for w2v + Random Forest on X_testN0 (1%split): 0.6973378490321086\n",
            "AUC score for w2v + Random Forest on X_testN1 (1%split): 0.654140904870103\n",
            "AUC score for w2v + Random Forest on X_testN2 (1%split): 0.6514404624546843\n",
            "AUC score for w2v + Random Forest on X_testN3 (1%split): 0.6495635691477383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results Random Forest with FastText. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "kXxdZSVDOSEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_RF_ft.pkl', 'rb') as f:\n",
        "  Model_RF_ft = load(f)\n",
        "  score_test = Model_RF_ft.score(X_train_ft, y_train)\n",
        "  print(f'Model overfit check (ft + RF): {score_test}') #.9994..\n",
        "  for i, name in zip(ListTestsets_ft, names_list):\n",
        "    y_pred = Model_RF_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_RF_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for ft + Random Forest on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP3D-lCkN6QL",
        "outputId": "bc0173df-3d5a-4f39-efc9-b3e0ed800907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (ft + RF): 0.9983\n",
            "AUC score for ft + Random Forest on X_testN0 (1%split): 0.7019423688730679\n",
            "AUC score for ft + Random Forest on X_testN1 (1%split): 0.6574926847333841\n",
            "AUC score for ft + Random Forest on X_testN2 (1%split): 0.6549004898842512\n",
            "AUC score for ft + Random Forest on X_testN3 (1%split): 0.6577957000251531\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 4: SupportVectorMachine"
      ],
      "metadata": {
        "id": "mPlTuQSZCBsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.linear_model import SGDClassifier\n",
        "classifierSVM = SGDClassifier(loss='hinge', penalty='l2', random_state=42)"
      ],
      "metadata": {
        "id": "tP4yD0RjZaFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\n",
        "Model_SVM_BoW = classifierSVM.fit(X_train_BoW, y_train)\n",
        "with open(picklepath+\"Model_SVM_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_SVM_BoW, f)\n",
        "\n",
        "Model_SVM_HV = classifierSVM.fit(X_train_HV_scaled, y_train)\n",
        "with open(picklepath+\"Model_SVM_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_SVM_HV, f)\n",
        "\n",
        "Model_SVM_TFIDF = classifierSVM.fit(X_train_TFIDF, y_train)\n",
        "with open(picklepath+\"Model_SVM_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_SVM_TFIDF, f)\n",
        "\n",
        "Model_SVM_GloVe = classifierSVM.fit(X_train_GloVe_scaled, y_train)\n",
        "with open(picklepath+\"Model_SVM_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_SVM_GloVe, f)"
      ],
      "metadata": {
        "id": "jT91SvXnZn_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_SVM_w2v = classifierSVM.fit(X_train_w2v, y_train)\n",
        "with open(picklepath+\"Model_SVM_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_SVM_w2v, f)"
      ],
      "metadata": {
        "id": "bgq0TnrNOkJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_SVM_ft = classifierSVM.fit(X_train_ft, y_train)\n",
        "with open(picklepath+\"Model_SVM_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_SVM_ft, f)"
      ],
      "metadata": {
        "id": "jlkv1-U6OkT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results SVM with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "CCCnAKxGEIIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_SVM_BoW.pkl', 'rb') as f:\n",
        "  Model_SVM_BoW = load(f)\n",
        "  score_test = Model_SVM_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + SVM): {score_test}') #.9876..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_SVM_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_SVM_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + SVM on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3trow8rEAO8",
        "outputId": "9ae5cd84-d853-4950-d548-87eb63cd9759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (BoW + SVM): 0.9939\n",
            "AUC score for BoW + SVM on X_testN0 (1%split): 0.8766864532467383\n",
            "AUC score for BoW + SVM on X_testN1 (1%split): 0.8422295077446273\n",
            "AUC score for BoW + SVM on X_testN2 (1%split): 0.8384163850901329\n",
            "AUC score for BoW + SVM on X_testN3 (1%split): 0.8381984770960398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results SVM with HV. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "SITQpNQrd7BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_SVM_HV.pkl', 'rb') as f:\n",
        "  Model_SVM_HV = load(f)\n",
        "  score_test = Model_SVM_HV.score(X_train_HV_scaled, y_train)\n",
        "  print(f'Model overfit check (HV + SVM): {score_test}') #.9110..\n",
        "  for i, name in zip(ListTestsets_HV_scaled, names_list):\n",
        "    y_pred = Model_SVM_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_SVM_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HashingVectors + SVM on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNitmSf9d9RT",
        "outputId": "e98f30ee-1ba1-49bd-d1a1-68a422da081e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (HV + SVM): 0.9981\n",
            "AUC score for HashingVectors + SVM on X_testN0 (1%split): 0.871076087871468\n",
            "AUC score for HashingVectors + SVM on X_testN1 (1%split): 0.8341981357884718\n",
            "AUC score for HashingVectors + SVM on X_testN2 (1%split): 0.8336172425388025\n",
            "AUC score for HashingVectors + SVM on X_testN3 (1%split): 0.8320987064337105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results SVM with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "jgL2QINvEL4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_SVM_TFIDF.pkl', 'rb') as f:\n",
        "  Model_SVM_TFIDF = load(f)\n",
        "  score_test = Model_SVM_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + SVM): {score_test}') #.9408..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_SVM_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_SVM_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + SVM on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFpP-sFhEFKb",
        "outputId": "a370b730-922a-4bde-b8ea-a05d959a6c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (TFIDF + SVM): 0.9739\n",
            "AUC score for TFIDF + SVM on X_testN0 (1%split): 0.9045909613423023\n",
            "AUC score for TFIDF + SVM on X_testN1 (1%split): 0.8678506824634915\n",
            "AUC score for TFIDF + SVM on X_testN2 (1%split): 0.865347638762019\n",
            "AUC score for TFIDF + SVM on X_testN3 (1%split): 0.8635709550408571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results SVM with GloVe. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "cCV5SQLUeKXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_SVM_GloVe.pkl', 'rb') as f:\n",
        "  Model_SVM_GloVe = load(f)\n",
        "  score_test = Model_SVM_GloVe.score(X_train_GloVe_scaled, y_train)\n",
        "  print(f'Model overfit check (TFIDF + SVM): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_GloVe_scaled, names_list):\n",
        "    y_pred = Model_SVM_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_SVM_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe + SVM on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKth3aqTeLsG",
        "outputId": "889d5c7b-db2d-42a3-8136-edf537baf8a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (TFIDF + SVM): 0.7924\n",
            "AUC score for GloVe + SVM on X_testN0 (1%split): 0.7060744999145571\n",
            "AUC score for GloVe + SVM on X_testN1 (1%split): 0.6768107113100634\n",
            "AUC score for GloVe + SVM on X_testN2 (1%split): 0.6778889981078263\n",
            "AUC score for GloVe + SVM on X_testN3 (1%split): 0.676890995819438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results SVM with Word2Vec. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "f5q_OUw_P4pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_SVM_w2v.pkl', 'rb') as f:\n",
        "  Model_SVM_w2v = load(f)\n",
        "  score_test = Model_SVM_w2v.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (w2v + SVM): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_w2v, names_list):\n",
        "    y_pred = Model_SVM_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_SVM_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for w2v + SVM on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuGsOQkFO1OZ",
        "outputId": "ad2328ce-dee2-4403-9f12-3ddc0eef20cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (w2v + SVM): 0.9598\n",
            "AUC score for w2v + SVM on X_testN0 (1%split): 0.7156686292984323\n",
            "AUC score for w2v + SVM on X_testN1 (1%split): 0.676971267898946\n",
            "AUC score for w2v + SVM on X_testN2 (1%split): 0.6737404513556419\n",
            "AUC score for w2v + SVM on X_testN3 (1%split): 0.6743153036900678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results SVM with FastText. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "LhbglL14P9E_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_SVM_ft.pkl', 'rb') as f:\n",
        "  Model_SVM_ft = load(f)\n",
        "  score_test = Model_SVM_ft.score(X_train_ft, y_train)\n",
        "  print(f'Model overfit check (ft + SVM): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_ft, names_list):\n",
        "    y_pred = Model_SVM_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_SVM_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for FastText + SVM on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uBveU5NO1bT",
        "outputId": "4d88b431-87cb-41f6-9be2-09da33996e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (ft + SVM): 0.9511\n",
            "AUC score for FastText + SVM on X_testN0 (1%split): 0.7326993159380276\n",
            "AUC score for FastText + SVM on X_testN1 (1%split): 0.7033126166677651\n",
            "AUC score for FastText + SVM on X_testN2 (1%split): 0.6980427302026654\n",
            "AUC score for FastText + SVM on X_testN3 (1%split): 0.700528674550771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 5: K NearestNeighbour (KNN)"
      ],
      "metadata": {
        "id": "M7r-wRjyzMwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifierKNN = KNeighborsClassifier(n_neighbors=3)\n",
        "#remember: training KNN is fast, scoring is slow and RAM intensive."
      ],
      "metadata": {
        "id": "KZKZzj9EzS9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\n",
        "Model_KNN_BoW = classifierKNN.fit(X_train_BoW, y_train)\n",
        "with open(picklepath+\"Model_KNN_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_BoW, f)\n",
        "\n",
        "Model_KNN_HV = classifierKNN.fit(X_train_HV, y_train)\n",
        "with open(picklepath+\"Model_KNN_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_HV, f)\n",
        "\n",
        "Model_KNN_TFIDF = classifierKNN.fit(X_train_TFIDF, y_train)\n",
        "with open(picklepath+\"Model_KNN_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_TFIDF, f)\n",
        "\n",
        "Model_KNN_GloVe = classifierKNN.fit(X_train_GloVe, y_train)\n",
        "with open(picklepath+\"Model_KNN_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_GloVe, f)"
      ],
      "metadata": {
        "id": "WgRzZ7Ppzht4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_KNN_w2v = classifierKNN.fit(X_train_w2v, y_train)\n",
        "with open(picklepath+\"Model_KNN_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_w2v, f)"
      ],
      "metadata": {
        "id": "L3_vewHjQDLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_KNN_ft = classifierKNN.fit(X_train_ft, y_train)\n",
        "with open(picklepath+\"Model_KNN_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_ft, f)"
      ],
      "metadata": {
        "id": "fUfHuToXQDWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results KNN with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "bp8BrZqy0mbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_KNN_BoW.pkl', 'rb') as f:\n",
        "  Model_KNN_BoW = load(f)\n",
        "  score_test = Model_KNN_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + KNN): {score_test}') #.9096..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_KNN_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_KNN_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + KNN on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp638Aq301Ls",
        "outputId": "29d015df-fd9d-43d5-d321-2b32f18ce26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (BoW + KNN): 0.8511\n",
            "AUC score for BoW + KNN on X_testN0 (1%split): 0.732151208533571\n",
            "AUC score for BoW + KNN on X_testN1 (1%split): 0.6956587977710234\n",
            "AUC score for BoW + KNN on X_testN2 (1%split): 0.6925319902094415\n",
            "AUC score for BoW + KNN on X_testN3 (1%split): 0.6907615421381017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results KNN with HV. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "vuN9-EFL0o7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_KNN_HV.pkl', 'rb') as f:\n",
        "  Model_KNN_HV = load(f)\n",
        "  score_test = Model_KNN_HV.score(X_train_HV, y_train)\n",
        "  print(f'Model overfit check (HV + KNN): {score_test}') #.8668..\n",
        "  for i, name in zip(ListTestsets_HV, names_list):\n",
        "    y_pred = Model_KNN_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_KNN_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HashingVectors + KNN on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XttRUvWq3dSQ",
        "outputId": "6058cb6c-d84d-4b17-d4ad-8469306e782e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (HV + KNN): 0.8851\n",
            "AUC score for HashingVectors + KNN on X_testN0 (1%split): 0.7590449944437667\n",
            "AUC score for HashingVectors + KNN on X_testN1 (1%split): 0.73010039653098\n",
            "AUC score for HashingVectors + KNN on X_testN2 (1%split): 0.7290034027837499\n",
            "AUC score for HashingVectors + KNN on X_testN3 (1%split): 0.7310454931631674\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results KNN with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "EGQhXUiN0rye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_KNN_TFIDF.pkl', 'rb') as f:\n",
        "  Model_KNN_TFIDF = load(f)\n",
        "  score_test = Model_KNN_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + KNN): {score_test}') #.8907..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_KNN_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_KNN_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + KNN on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zMo5Gs97PmT",
        "outputId": "cc050248-7b06-4035-c071-b830a4dc0fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (TFIDF + KNN): 0.8904\n",
            "AUC score for TFIDF + KNN on X_testN0 (1%split): 0.7861886993982403\n",
            "AUC score for TFIDF + KNN on X_testN1 (1%split): 0.763928270619877\n",
            "AUC score for TFIDF + KNN on X_testN2 (1%split): 0.7613081127138329\n",
            "AUC score for TFIDF + KNN on X_testN3 (1%split): 0.7594217685648637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results KNN with GloVe. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "vmkej_570u1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_KNN_GloVe.pkl', 'rb') as f:\n",
        "  Model_KNN_GloVe = load(f)\n",
        "  score_test = Model_KNN_GloVe.score(X_train_GloVe, y_train)\n",
        "  print(f'Model overfit check (GloVe + KNN): {score_test}') #.8720..\n",
        "  for i, name in zip(ListTestsets_GloVe, names_list):\n",
        "    y_pred = Model_KNN_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_KNN_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe + KNN on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29UGNMdX8ao4",
        "outputId": "ac919b42-0e34-4607-f9c7-ffe9f66d70d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (GloVe + KNN): 0.7856\n",
            "AUC score for GloVe + KNN on X_testN0 (1%split): 0.6037559634978897\n",
            "AUC score for GloVe + KNN on X_testN1 (1%split): 0.5802331491652458\n",
            "AUC score for GloVe + KNN on X_testN2 (1%split): 0.5805681713639106\n",
            "AUC score for GloVe + KNN on X_testN3 (1%split): 0.5835420584180866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results KNN with Word2Vec. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "lPYLukngPuX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_KNN_w2v.pkl', 'rb') as f:\n",
        "  Model_KNN_w2v = load(f)\n",
        "  score_test = Model_KNN_w2v.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (w2v + KNN): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_w2v, names_list):\n",
        "    y_pred = Model_KNN_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_KNN_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for w2v + KNN on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhapW_2YPU9j",
        "outputId": "4f582843-43a6-46c3-e5ba-ed9e6856a2c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (w2v + KNN): 0.687\n",
            "AUC score for w2v + KNN on X_testN0 (1%split): 0.5020153330195813\n",
            "AUC score for w2v + KNN on X_testN1 (1%split): 0.498937436983683\n",
            "AUC score for w2v + KNN on X_testN2 (1%split): 0.4935782046363436\n",
            "AUC score for w2v + KNN on X_testN3 (1%split): 0.4954335652707799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results KNN with FastText. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "thfRtkFYPwwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_KNN_ft.pkl', 'rb') as f:\n",
        "  Model_KNN_ft = load(f)\n",
        "  score_test = Model_KNN_ft.score(X_train_ft, y_train)\n",
        "  print(f'Model overfit check (ft + KNN): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_ft, names_list):\n",
        "    y_pred = Model_KNN_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_KNN_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for FastText + KNN on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOJheSBfPVve",
        "outputId": "c9d4a9ee-2d24-49c6-9a27-dac310e3507c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (ft + KNN): 0.7712\n",
            "AUC score for FastText + KNN on X_testN0 (1%split): 0.5993622376239505\n",
            "AUC score for FastText + KNN on X_testN1 (1%split): 0.5720117825524413\n",
            "AUC score for FastText + KNN on X_testN2 (1%split): 0.5763466571505816\n",
            "AUC score for FastText + KNN on X_testN3 (1%split): 0.576207620804342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test with scaled dataset.."
      ],
      "metadata": {
        "id": "Vcx2T8YOFjhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Using scaled GloVe instead:\n",
        "'''\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifierKNN = KNeighborsClassifier(n_neighbors=2)\n",
        "\n",
        "Model_KNN_GloVe_scaled = classifierKNN.fit(X_train_GloVe_scaled, y_train)\n",
        "with open(picklepath+\"Model_KNN_GloVe_scaled.pkl\", \"wb\") as f:\n",
        "    dump(Model_KNN_GloVe_scaled, f)\n",
        "'''"
      ],
      "metadata": {
        "id": "75lpeTgVFRXi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "56805760-56e3-476e-ae3d-c68f7443b616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.neighbors import KNeighborsClassifier\\nclassifierKNN = KNeighborsClassifier(n_neighbors=2)\\n\\nModel_KNN_GloVe_scaled = classifierKNN.fit(X_train_GloVe_scaled, y_train)\\nwith open(picklepath+\"Model_KNN_GloVe_scaled.pkl\", \"wb\") as f:\\n    dump(Model_KNN_GloVe_scaled, f)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "with open(picklepath + 'Model_KNN_GloVe_scaled.pkl', 'rb') as f:\n",
        "  Model_KNN_GloVe_scaled = load(f)\n",
        "  score_test = Model_KNN_GloVe_scaled.score(X_train_GloVe_scaled, y_train)\n",
        "  print(f'Model overfit check (GloVe_scaled + KNN): {score_test}') #.9876..\n",
        "  for i, name in zip(ListTestsets_GloVe_scaled, names_list):\n",
        "    score_test = Model_KNN_GloVe_scaled.score(i, y_testALL)\n",
        "    print(f'accuracy GloVe_scaled + KNN on {name} (1%split): {score_test}')\n",
        "'''\n",
        "#results:\n",
        "# Model overfit check (GloVe_scaled + KNN): 0.9476672828096119\n",
        "#accuracy GloVe_scaled + KNN on X_testN0 (1%split): 0.5467836257309941\n",
        "#accuracy GloVe_scaled + KNN on X_testN1 (1%split): 0.5423976608187134\n",
        "#accuracy GloVe_scaled + KNN on X_testN2 (1%split): 0.5511695906432749\n",
        "#accuracy GloVe_scaled + KNN on X_testN3 (1%split): 0.5248538011695907"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "M3t6Hz3oFnxd",
        "outputId": "e3e86baf-3eb1-48ab-dc7f-4f7ba5fe2cc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nwith open(picklepath + 'Model_KNN_GloVe_scaled.pkl', 'rb') as f:\\n  Model_KNN_GloVe_scaled = load(f)\\n  score_test = Model_KNN_GloVe_scaled.score(X_train_GloVe_scaled, y_train)\\n  print(f'Model overfit check (GloVe_scaled + KNN): {score_test}') #.9876..\\n  for i, name in zip(ListTestsets_GloVe_scaled, names_list):\\n    score_test = Model_KNN_GloVe_scaled.score(i, y_testALL)\\n    print(f'accuracy GloVe_scaled + KNN on {name} (1%split): {score_test}')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 6. GradientBoosting"
      ],
      "metadata": {
        "id": "GloPNlsOd99p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.ensemble import GradientBoostingClassifier"
      ],
      "metadata": {
        "id": "S-INAnSFd9sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifierGB = GradientBoostingClassifier(learning_rate=0.3, max_depth=2)\n",
        "#change from 0.1 to 0.2 to save time when testing.. (NOTE: takes long time still at .2..)"
      ],
      "metadata": {
        "id": "svmHMKIvebjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\n",
        "Model_GB_BoW = classifierGB.fit(X_train_BoW, y_train)\n",
        "with open(picklepath+\"Model_GB_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_GB_BoW, f)\n",
        "\n",
        "Model_GB_HV = classifierGB.fit(X_train_HV, y_train)\n",
        "with open(picklepath+\"Model_GB_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_GB_HV, f)\n",
        "\n",
        "Model_GB_TFIDF = classifierGB.fit(X_train_TFIDF, y_train)\n",
        "with open(picklepath+\"Model_GB_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_GB_TFIDF, f)\n",
        "\n",
        "Model_GB_GloVe = classifierGB.fit(X_train_GloVe, y_train)\n",
        "with open(picklepath+\"Model_GB_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_GB_GloVe, f)"
      ],
      "metadata": {
        "id": "5Eil_0QzeS5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_GB_w2v = classifierGB.fit(X_train_w2v, y_train)\n",
        "with open(picklepath+\"Model_GB_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_GB_w2v, f)"
      ],
      "metadata": {
        "id": "xt6lTeunQ-hk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "2be2d897-b62b-4117-e4b3-7a6c4e100c69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-0de61279000b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModel_GB_w2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifierGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklepath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"Model_GB_w2v.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel_GB_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    664\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \"\"\"\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    418\u001b[0m             )\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Model_GB_ft = classifierGB.fit(X_train_ft, y_train)\n",
        "with open(picklepath+\"Model_GB_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_GB_ft, f)"
      ],
      "metadata": {
        "id": "DKAn-tDzQ_PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results GradientBoosting with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "lKXfgs_sfZOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_GB_BoW.pkl', 'rb') as f:\n",
        "  Model_GB_BoW = load(f)\n",
        "  score_test = Model_GB_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + GB): {score_test}') #.9096..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_GB_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_GB_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + GB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "id": "JQubVBlkfEsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results GradientBoosting with HV. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "d2ZiSRoSmc-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_GB_HV.pkl', 'rb') as f:\n",
        "  Model_GB_HV = load(f)\n",
        "  score_test = Model_GB_HV.score(X_train_HV, y_train)\n",
        "  print(f'Model overfit check (HV + GB): {score_test}') #.8668..\n",
        "  for i, name in zip(ListTestsets_HV, names_list):\n",
        "    y_pred = Model_GB_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_GB_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HashingVectors + GB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "id": "GCg7p4Ncl4DK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results GradientBoosting with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "1ZV_MBAqmgkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_GB_TFIDF.pkl', 'rb') as f:\n",
        "  Model_GB_TFIDF = load(f)\n",
        "  score_test = Model_GB_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + GB): {score_test}') #.8907..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_GB_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_GB_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + GB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "id": "s0tZqoc8mHeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results GradientBoosting with GloVe. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "ucef3SA7mjMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_GB_GloVe.pkl', 'rb') as f:\n",
        "  Model_GB_GloVe = load(f)\n",
        "  score_test = Model_GB_GloVe.score(X_train_GloVe, y_train)\n",
        "  print(f'Model overfit check (GloVe + GB): {score_test}') #.8720..\n",
        "  for i, name in zip(ListTestsets_GloVe, names_list):\n",
        "    y_pred = Model_GB_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_GB_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe + GB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "id": "YRxrgmbpmTTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results GradientBoosting with Word2Vec. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "PjgPjkPxRryG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_GB_w2v.pkl', 'rb') as f:\n",
        "  Model_GB_w2v = load(f)\n",
        "  score_test = Model_GB_w2v.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (w2v + GB): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_w2v, names_list):\n",
        "    y_pred = Model_GB_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_GB_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for word2vec + GB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "id": "oy9S48erRTVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results GradientBoosting with FastText. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "QXn427sgRuXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_GB_ft.pkl', 'rb') as f:\n",
        "  Model_GB_ft = load(f)\n",
        "  score_test = Model_GB_ft.score(X_train_ft, y_train)\n",
        "  print(f'Model overfit check (ft + GB): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_ft, names_list):\n",
        "    y_pred = Model_GB_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_GB_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for FastText + GB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "id": "x5WM8WTbRgj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 7. XGBoost (extreme gradient boosting)"
      ],
      "metadata": {
        "id": "pG-N5mm8n42R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "ioHuBfdqoBNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "classifierXGB = XGBClassifier(learning_rate=0.2, max_depth=2)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Y3ZodrYroNQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the models and pickling each one separately\n",
        "\"\"\"\n",
        "Model_XGB_BoW = classifierXGB.fit(X_train_BoW, y_train)\n",
        "with open(picklepath+\"Model_XGB_BoW.pkl\", \"wb\") as f:\n",
        "    dump(Model_XGB_BoW, f)\n",
        "\n",
        "Model_XGB_HV = classifierXGB.fit(X_train_HV, y_train)\n",
        "with open(picklepath+\"Model_XGB_HV.pkl\", \"wb\") as f:\n",
        "    dump(Model_XGB_HV, f)\n",
        "\n",
        "Model_XGB_TFIDF = classifierXGB.fit(X_train_TFIDF, y_train)\n",
        "with open(picklepath+\"Model_XGB_TFIDF.pkl\", \"wb\") as f:\n",
        "    dump(Model_XGB_TFIDF, f)\n",
        "\n",
        "Model_XGB_GloVe = classifierXGB.fit(X_train_GloVe, y_train)\n",
        "with open(picklepath+\"Model_XGB_GloVe.pkl\", \"wb\") as f:\n",
        "    dump(Model_XGB_GloVe, f)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fklNuNG6oKRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model_XGB_w2v = classifierXGB.fit(X_train_w2v, y_train)\n",
        "with open(picklepath+\"Model_XGB_w2v.pkl\", \"wb\") as f:\n",
        "    dump(Model_XGB_w2v, f)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "-oTS7vK6R2Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model_XGB_ft = classifierXGB.fit(X_train_ft, y_train)\n",
        "with open(picklepath+\"Model_XGB_ft.pkl\", \"wb\") as f:\n",
        "    dump(Model_XGB_ft, f)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HmPBV_NHR2h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [(X_train_BoW, \"Model_XGB_BoW.pkl\"),\n",
        "              (X_train_HV, \"Model_XGB_HV.pkl\"),\n",
        "              (X_train_TFIDF, \"Model_XGB_TFIDF.pkl\"),\n",
        "              (X_train_GloVe, \"Model_XGB_GloVe.pkl\"),\n",
        "              (X_train_w2v, \"Model_XGB_w2v.pkl\"),\n",
        "              (X_train_ft, \"Model_XGB_ft.pkl\"),]"
      ],
      "metadata": {
        "id": "au-2s9Eu-THo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the XGBClassifier and dump each model to a corresponding pickle file\n",
        "for data, filename in train_data:\n",
        "    classifierXGB = XGBClassifier(learning_rate=0.3, max_depth=2) #.2 or .1 is better than .3, but .3 for speed atm.\n",
        "    model = classifierXGB.fit(data, y_train)\n",
        "    with open(picklepath + filename, \"wb\") as f:\n",
        "        dump(model, f)"
      ],
      "metadata": {
        "id": "EHZMhVJC-Km4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "d47e2f4c-08bd-48ae-9d90-2208fd35130b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-134-457bbb5e544e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclassifierXGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.2 or .1 is better than .3, but .3 for speed atm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifierXGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    726\u001b[0m                                     missing=self.missing, nthread=self.n_jobs)\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         self._Booster = train(xgb_options, train_dmatrix, self.get_num_boosting_rounds(),\n\u001b[0m\u001b[1;32m    729\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[1;32m    213\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0m\u001b[1;32m   1109\u001b[0m                                                     dtrain.handle))\n\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24.4GB"
      ],
      "metadata": {
        "id": "oXd3N4nmE6-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results XGB with BoW. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "QxSDoPclo6Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_XGB_BoW.pkl', 'rb') as f:\n",
        "  Model_XGB_BoW = load(f)\n",
        "  score_test = Model_XGB_BoW.score(X_train_BoW, y_train)\n",
        "  print(f'Model overfit check (BoW + XGB): {score_test}') #.9096..\n",
        "  for i, name in zip(ListTestsets_BoW, names_list):\n",
        "    y_pred = Model_XGB_BoW.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_XGB_BoW.score(i, y_testALL)\n",
        "    print(f'AUC score for BoW + XGB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "FlTZrDtrpCyP",
        "outputId": "0c834584-6e86-4652-b74e-118466f5ca2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-af5b6e902248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Model_XGB_BoW.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mModel_XGB_BoW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mscore_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_XGB_BoW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_BoW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model overfit check (BoW + XGB): {score_test}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.9096..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListTestsets_BoW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_ntree_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[0m\u001b[1;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1687\u001b[0m                             ', '.join(str(s) for s in my_missing))\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0m\u001b[1;32m   1690\u001b[0m                                             data.feature_names))\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194',...\ntraining data did not have the following fields: f29734, f29760, f29747, f29742, f29748, f29745, f29755, f29739, f29735, f29756, f29744, f29751, f29757, f29736, f29754, f29746, f29738, f29737, f29752, f29743, f29750, f29740, f29753, f29749, f29759, f29741, f29758, f29761"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results XGB with HV. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "ouq0UYtYpkKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_XGB_HV.pkl', 'rb') as f:\n",
        "  Model_XGB_HV = load(f)\n",
        "  score_test = Model_XGB_HV.score(X_train_HV, y_train)\n",
        "  print(f'Model overfit check (HV + XGB): {score_test}') #.8668..\n",
        "  for i, name in zip(ListTestsets_HV, names_list):\n",
        "    y_pred = Model_XGB_HV.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_XGB_HV.score(i, y_testALL)\n",
        "    print(f'AUC score for HashingVectors + XGB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nbi2A85pMC6",
        "outputId": "cf5b9ea4-ca8b-4087-973c-8d0b5e02445a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (HV + XGB): 0.89255\n",
            "AUC score for HashingVectors + XGB on X_testN0 (1%split): 0.8812027003172881\n",
            "AUC score for HashingVectors + XGB on X_testN1 (1%split): 0.8458730994464416\n",
            "AUC score for HashingVectors + XGB on X_testN2 (1%split): 0.8449783272014592\n",
            "AUC score for HashingVectors + XGB on X_testN3 (1%split): 0.8427102908474466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results XGB with TFIDF. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "Ue0Co7OLpnT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_XGB_TFIDF.pkl', 'rb') as f:\n",
        "  Model_XGB_TFIDF = load(f)\n",
        "  score_test = Model_XGB_TFIDF.score(X_train_TFIDF, y_train)\n",
        "  print(f'Model overfit check (TFIDF + XGB): {score_test}') #.8907..\n",
        "  for i, name in zip(ListTestsets_TFIDF, names_list):\n",
        "    y_pred = Model_XGB_TFIDF.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_XGB_TFIDF.score(i, y_testALL)\n",
        "    print(f'AUC score for TFIDF + XGB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "ck8yvHi_pTfU",
        "outputId": "c25dc1cc-a520-4b64-c342-e8a69fa40449"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-85f8f8aa85d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpicklepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Model_XGB_TFIDF.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mModel_XGB_TFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mscore_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_XGB_TFIDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_TFIDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Model overfit check (TFIDF + XGB): {score_test}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.8907..\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mListTestsets_TFIDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, validate_features)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mntree_limit\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mntree_limit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_ntree_limit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         class_probs = self.get_booster().predict(test_dmatrix,\n\u001b[0m\u001b[1;32m    789\u001b[0m                                                  \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                                                  \u001b[0mntree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mntree_limit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, output_margin, ntree_limit, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidate_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_validate_features\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1687\u001b[0m                             ', '.join(str(s) for s in my_missing))\n\u001b[1;32m   1688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m                 raise ValueError(msg.format(self.feature_names,\n\u001b[0m\u001b[1;32m   1690\u001b[0m                                             data.feature_names))\n\u001b[1;32m   1691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39', 'f40', 'f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49', 'f50', 'f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59', 'f60', 'f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69', 'f70', 'f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79', 'f80', 'f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89', 'f90', 'f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99', 'f100', 'f101', 'f102', 'f103', 'f104', 'f105', 'f106', 'f107', 'f108', 'f109', 'f110', 'f111', 'f112', 'f113', 'f114', 'f115', 'f116', 'f117', 'f118', 'f119', 'f120', 'f121', 'f122', 'f123', 'f124', 'f125', 'f126', 'f127', 'f128', 'f129', 'f130', 'f131', 'f132', 'f133', 'f134', 'f135', 'f136', 'f137', 'f138', 'f139', 'f140', 'f141', 'f142', 'f143', 'f144', 'f145', 'f146', 'f147', 'f148', 'f149', 'f150', 'f151', 'f152', 'f153', 'f154', 'f155', 'f156', 'f157', 'f158', 'f159', 'f160', 'f161', 'f162', 'f163', 'f164', 'f165', 'f166', 'f167', 'f168', 'f169', 'f170', 'f171', 'f172', 'f173', 'f174', 'f175', 'f176', 'f177', 'f178', 'f179', 'f180', 'f181', 'f182', 'f183', 'f184', 'f185', 'f186', 'f187', 'f188', 'f189', 'f190', 'f191', 'f192', 'f193', 'f194',...\ntraining data did not have the following fields: f29734, f29760, f29747, f29742, f29748, f29745, f29755, f29739, f29735, f29756, f29744, f29751, f29757, f29736, f29754, f29746, f29738, f29737, f29752, f29743, f29750, f29740, f29753, f29749, f29759, f29741, f29758, f29761"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results XGB with GloVe. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "xuFHQx3IpqR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_XGB_GloVe.pkl', 'rb') as f:\n",
        "  Model_XGB_GloVe = load(f)\n",
        "  score_test = Model_XGB_GloVe.score(X_train_GloVe, y_train)\n",
        "  print(f'Model overfit check (GloVe + XGB): {score_test}') #.8720..\n",
        "  for i, name in zip(ListTestsets_GloVe, names_list):\n",
        "    y_pred = Model_XGB_GloVe.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_XGB_GloVe.score(i, y_testALL)\n",
        "    print(f'AUC score for GloVe + XGB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTgG82glpbzI",
        "outputId": "7b22cba4-c8d6-46a7-b0c9-bfb9f871b899"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (GloVe + XGB): 0.79775\n",
            "AUC score for GloVe + XGB on X_testN0 (1%split): 0.7821437390123086\n",
            "AUC score for GloVe + XGB on X_testN1 (1%split): 0.7472891206397954\n",
            "AUC score for GloVe + XGB on X_testN2 (1%split): 0.7451850632313123\n",
            "AUC score for GloVe + XGB on X_testN3 (1%split): 0.7468659706858332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results XGB with Word2Vec. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "E0JepJngSfZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_XGB_w2v.pkl', 'rb') as f:\n",
        "  Model_XGB_w2v = load(f)\n",
        "  score_test = Model_XGB_w2v.score(X_train_w2v, y_train)\n",
        "  print(f'Model overfit check (w2v + XGB): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_w2v, names_list):\n",
        "    y_pred = Model_XGB_w2v.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_XGB_w2v.score(i, y_testALL)\n",
        "    print(f'AUC score for word2vec + XGB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xqSS96NSJg1",
        "outputId": "7fc30582-d8fc-4606-c903-6495bfd742ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (w2v + XGB): 0.7963\n",
            "AUC score for word2vec + XGB on X_testN0 (1%split): 0.7824904038528344\n",
            "AUC score for word2vec + XGB on X_testN1 (1%split): 0.7415777667404243\n",
            "AUC score for word2vec + XGB on X_testN2 (1%split): 0.7403308729434102\n",
            "AUC score for word2vec + XGB on X_testN3 (1%split): 0.7381865759129501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing results XGB with FastText. \n",
        "\n",
        "- (N0 (base), N1, N2 and N3)"
      ],
      "metadata": {
        "id": "D6cCx5MuSh38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(picklepath + 'Model_XGB_ft.pkl', 'rb') as f:\n",
        "  Model_XGB_ft = load(f)\n",
        "  score_test = Model_XGB_ft.score(X_train_ft, y_train)\n",
        "  print(f'Model overfit check (ft + XGB): {score_test}') #.7574..\n",
        "  for i, name in zip(ListTestsets_ft, names_list):\n",
        "    y_pred = Model_XGB_ft.predict(i)\n",
        "    score_test = roc_auc_score(y_testALL, y_pred)\n",
        "    #score_test = Model_XGB_ft.score(i, y_testALL)\n",
        "    print(f'AUC score for FastText + XGB on {name} (1%split): {score_test}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwxLi0i_SJvv",
        "outputId": "baa73be4-c66c-4445-942a-50f389117a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model overfit check (ft + XGB): 0.7928\n",
            "AUC score for FastText + XGB on X_testN0 (1%split): 0.7832968536079403\n",
            "AUC score for FastText + XGB on X_testN1 (1%split): 0.7443814391992487\n",
            "AUC score for FastText + XGB on X_testN2 (1%split): 0.7406441677349862\n",
            "AUC score for FastText + XGB on X_testN3 (1%split): 0.7419373420692206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noise (Lexical, BT-induced)"
      ],
      "metadata": {
        "id": "jt6KkLPeQwQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pathJWdist = '/content/drive/MyDrive/MYDATA/LexicalDistanceJW.csv'\n",
        "pathHamdist = '/content/drive/MyDrive/MYDATA/LexicalDistanceHAMMING.csv'"
      ],
      "metadata": {
        "id": "GMCbgIRPQ4kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_JaroWinkler = pd.read_csv(pathJWdist)\n",
        "df_Hamming = pd.read_csv(pathHamdist)"
      ],
      "metadata": {
        "id": "mfHeEWYaSCBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display(df_JaroWinkler.describe())"
      ],
      "metadata": {
        "id": "E9_EAMyYSdVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display(df_Hamming.describe())"
      ],
      "metadata": {
        "id": "9GZndHDmSonx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#values taken from the describe method commented out above\n",
        "\n",
        "# line 1 points\n",
        "x1 = [1,2,3]\n",
        "y1 = [0.136735,0.101772,0.069857]\n",
        "# plotting the line 1 points \n",
        "plt.plot(x1, y1, label = \"Hamming Distance\")\n",
        "# line 2 points\n",
        "x2 = [1,2,3]\n",
        "y2 = [0.136735,0.077469,0.054566]\n",
        "# plotting the line 2 points \n",
        "plt.plot(x2, y2, label = \"Jaro-Winkler Distance\")\n",
        "plt.xlabel('back-translation')\n",
        "# Set the y axis label of the current axis.\n",
        "plt.ylabel('Mean of Doc Distance (normalized)')\n",
        "# Set a title of the current axes.\n",
        "plt.title('Hamming and JaroWinkler for lexixal distance between backtranslation levels')\n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        "# Display a figure.\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "iLXa6QCQTE6c",
        "outputId": "4c825550-a70f-4859-8da8-f308872c4e1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Table)"
      ],
      "metadata": {
        "id": "oqR950o0vcNk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#total RAM (at 50dim, 80 maxlength, train at 20k, exlcuding KNN) is : 25.6GB\n",
        "#total RAM (at 50dim, 80 maxlength, train at 20k, exlcuding KNN pretrained models) is : 27GB\n",
        "#total RAM (at 50dim, 80 maxlength, train at 20k) is 29.5GB:"
      ],
      "metadata": {
        "id": "9qX-Xe8DvekL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Algorithm | Vectorizer | N0 | N1 | N2 | N3 |\n",
        "| --- | --- | --- | --- | --- | --- |\n",
        "| Logistic Regression | BoW | .843 | .843 |  .843 |  .843 |\n",
        "| Logistic Regression | HV | .843 | .843 |  .843 |  .843 |\n",
        "| Logistic Regression | TFIDF | .843 | .843 |  .843 |  .843 |\n",
        "| Logistic Regression | GloVe | .843 | .843 |  .843 |  .843 |\n",
        "| Logistic Regression | w2v | .843 | .843 |  .843 |  .843 |\n",
        "| Logistic Regression | FastText | .843 | .843 |  .843 |  .843 |\n",
        "| Naive Bayes | BoW | .843 | .843 |  .843 |  .843 |\n",
        "| Naive Bayes | HV | .843 | .843 |  .843 |  .843 |\n",
        "| Naive Bayes | TFIDF | .843 | .843 |  .843 |  .843 |\n",
        "| Naive Bayes | GloVe | .843 | .843 |  .843 |  .843 |\n",
        "| Naive Bayes | w2v | .843 | .843 |  .843 |  .843 |\n",
        "| Naive Bayes | FastText | .843 | .843 |  .843 |  .843 |\n",
        "| Random Forest | BoW | .843 | .843 |  .843 |  .843 |\n",
        "| Random Forest | HV | .843 | .843 |  .843 |  .843 |\n",
        "| Random Forest | TFIDF | .843 | .843 |  .843 |  .843 |\n",
        "| Random Forest | GloVe | .843 | .843 |  .843 |  .843 |\n",
        "| Random Forest | w2v | .843 | .843 |  .843 |  .843 |\n",
        "| Random Forest | FastText | .843 | .843 |  .843 |  .843 |\n",
        "| etc | etc | .843 | .843 |  .843 |  .843 |\n"
      ],
      "metadata": {
        "id": "4JbIeYPTPgsg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ma3icxuvgc20"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}