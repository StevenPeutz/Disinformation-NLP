{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "npQa-8kdeoYA",
        "bUqVxed90h2E"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNFrER9KBXZe7wylANEKHtq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenPeutz/Masterthesis-Disinformation-NLP/blob/master/CODE/2_DimReduction_Word2Vec_FastText(GH).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension reduction for pretrained embeddings\n",
        "- FastText (100 -> 50)\n",
        "- Word2Vec (100 -> 50)\n",
        "- (for GloVe the 50dim version is used), so no reduction required)\n",
        "\n",
        "\n",
        "<br>\n",
        "This is done for the following reasons;\n",
        "\n",
        "*   Comparison of architectures is considered more fair if all pretrained embeddings contains equal dimensions.\n",
        "*   RAM reductions (local 16GB, google colab 35).\n",
        "*   Reduction in storage space of embedding files (github limits).\n",
        "*   This allows all embedding and model combinations to be tested within a single 'overview' environment. Of course this comes at the cost of classification performance. Therefor all embedding and model combination are also run in seperate environments where RAM will not be a limiting factor.  \n",
        "<br>\n",
        "<br>\n",
        "The technique used for dimension reduction is PCA in the case of Word2Vec, and SVD in the case of FastText. (For FastText I have also used a PCA reduced version to make sure the method difference was not a big factor)\n",
        "\n"
      ],
      "metadata": {
        "id": "NDGr6QCS1Tav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "npQa-8kdeoYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "047-JxuBwrZg",
        "outputId": "c945f718-4e66-4ac7-9baf-bdbb28417826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY5szcu0wpPU"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import io\n",
        "import shutil"
      ],
      "metadata": {
        "id": "OJSqiYaA9ejf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWh54MMa-V4W",
        "outputId": "81ec3d0a-735f-444c-8737-0cd91e3c18fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext) (1.22.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4402288 sha256=74b7076be46ff5c3523c931fd873cc638e261333db9a36184cadf80832c9190a\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/61/2a/c54711a91c418ba06ba195b1d78ff24fcaad8592f2a694ac94\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "0N5011lUOvOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing FastText (300d-1M.bin)"
      ],
      "metadata": {
        "id": "bUqVxed90h2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For FastText I will use the built-in utility for dimension reduction instead of PCA. (The FastText built in method for dimension reduction is similar in principle but uses SVD method.)"
      ],
      "metadata": {
        "id": "2e-YdGam-kNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "\n",
        "#from drive storage:\n",
        "ft = fasttext.load_model('/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/cc.en.300.bin')\n",
        "\n",
        "#the above file can be downloaded from facebook AI research ('FAIR') with this URL:\n",
        "#'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin')\n",
        "\n",
        "print(ft.get_dimension())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRKuWRB69dxS",
        "outputId": "63077df5-2d7a-4d5f-dcae-b4cc011cf1d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fasttext.util.reduce_model(ft, 50)\n",
        "#print(ft.get_dimension())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuzWKSri9sUN",
        "outputId": "3e5cea5f-2917-40c4-9b24-e2a6a548dd14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft.save_model(\"FastText_SVD_Reduced50dim.bin\")"
      ],
      "metadata": {
        "id": "n-79mMF8CnKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ft_ft = fasttext.load_model(\"FastText_SVD_Reduced50dim.bin\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOc5Oi3EFbAQ",
        "outputId": "5d990c89-065e-41b4-899d-440adf142240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_ft.save_model('FastText_SVD_Reduced50dim.txt')"
      ],
      "metadata": {
        "id": "ouVFc1AjEKvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('FastText_SVD_Reduced50dim.txt', 'rb') as f_in, gzip.open('FastText_SVD_Reduced50dim.txt.gz', 'wb') as f_out:\n",
        "    shutil.copyfileobj(f_in, f_out)"
      ],
      "metadata": {
        "id": "XfUH9U1gM2u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp FastText_SVD_Reduced50dim.txt.gz /content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/"
      ],
      "metadata": {
        "id": "JK3h1pEjN5Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing Word2Vec (w2v.bin)"
      ],
      "metadata": {
        "id": "DLKfLTue0soQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from drive storage\n",
        "model_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v.bin'\n",
        "#the file could be downloaded from 'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin' but this is longer supported.\n",
        "#it is still available for download through kaggele (https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300)\n",
        "model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "metadata": {
        "id": "2keT9m3YHN84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the word vectors from the model\n",
        "word_vectors = model.vectors\n",
        "\n",
        "# Reduce the dimensionality of the vectors to 50 using PCA\n",
        "pca = PCA(n_components=50)\n",
        "word_vectors_50d = pca.fit_transform(word_vectors)"
      ],
      "metadata": {
        "id": "o5Wuv7XG1B4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the reduced vectors to a file in text format #SLOW!\n",
        "with gzip.open(\"/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v_PCA_reduced-vectors.txt.gz\", \"wt\") as f:\n",
        "    for i, word in enumerate(model.index2word):\n",
        "        vector_str = \" \".join([str(x) for x in word_vectors_50d[i]])\n",
        "        f.write(f\"{word} {vector_str}\\n\")"
      ],
      "metadata": {
        "id": "7IYM3kKQ1DTj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "863845dd-d32d-4352-9a95-7d0f0679eb7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-02176f86b58f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v_PCA_reduced-vectors.txt.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvector_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_vectors_50d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{word} {vector_str}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-02176f86b58f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v_PCA_reduced-vectors.txt.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wt\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mvector_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_vectors_50d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{word} {vector_str}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   open() <-gzip.open()\n",
        "*   wt = write text\n",
        "*   rt = read text\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Di5OXBT96Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQ9zR5DZcNVs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}