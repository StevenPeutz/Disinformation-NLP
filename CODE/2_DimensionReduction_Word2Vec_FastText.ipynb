{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8lo300DNm6jzIQAXdoEEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StevenPeutz/Masterthesis-Disinformation-NLP/blob/master/CODE/2_DimensionReduction_Word2Vec_FastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimension reduction for pretrained embeddings\n",
        "- FastText (100 -> 50)\n",
        "- Word2Vec (100 -> 50)\n",
        "- (for GloVe the 50dim version is used), so no reduction required)\n",
        "\n",
        "\n",
        "<br>\n",
        "This is done for the following reasons;\n",
        "\n",
        "*   Comparison of architectures is considered more far if all pretrained embeddings contains equal dimensions.\n",
        "*   RAM reductions (local 16GB, google colab 32).\n",
        "*   Reduction in storage space of embedding files (github limits).\n",
        "*   This allows all embedding and model combinations to be tested within a single 'overview' environment. Of course this comes at the cost of classification performance. Therefor all embedding and model combination are also run in seperate environments where RAM will not be a limiting factor.  \n",
        "<br>\n",
        "<br>\n",
        "The technique used for dimension reduction is PCA.\n",
        "\n"
      ],
      "metadata": {
        "id": "NDGr6QCS1Tav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "047-JxuBwrZg",
        "outputId": "ac8b8a5a-b974-4800-d095-4cd119b04c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY5szcu0wpPU"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import io"
      ],
      "metadata": {
        "id": "OJSqiYaA9ejf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing FastText (wiki-news-300d-1M.vec)"
      ],
      "metadata": {
        "id": "bUqVxed90h2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/wiki-news-300d-1M.vec.gz'\n",
        "#model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
        "with gzip.open(model_path, 'rb') as f:\n",
        "    model = KeyedVectors.load_word2vec_format(f, binary=False)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NecSgDGVwwkv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "020f2bdb-d13f-4d2f-f9e5-309fa8372165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nmodel_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/wiki-news-300d-1M.vec.gz'\\n#model = KeyedVectors.load_word2vec_format(model_path, binary=False)\\nwith gzip.open(model_path, 'rb') as f:\\n    model = KeyedVectors.load_word2vec_format(f, binary=False)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/wiki-news-300d-1M.vec.'\n",
        "model = KeyedVectors.load_word2vec_format(model_path, binary=False)"
      ],
      "metadata": {
        "id": "Ya2fNvoAHuVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the word vectors from the model\n",
        "word_vectors = model.vectors\n",
        "\n",
        "# Reduce the dimensionality of the vectors to 50 using PCA\n",
        "pca = PCA(n_components=50)\n",
        "word_vectors_50d = pca.fit_transform(word_vectors)"
      ],
      "metadata": {
        "id": "F0pEYbmYwx_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the reduced vectors to a file in text format\n",
        "\"\"\"\n",
        "with open(\"/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/PCA_reduced-vectors.txt\", \"w\") as f:\n",
        "    for i, word in enumerate(model.index2word):\n",
        "        vector_str = \" \".join([str(x) for x in word_vectors_50d[i]])\n",
        "        f.write(f\"{word} {vector_str}\\n\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ymZh_SMqxgo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the reduced vectors to a file in compressed gzip format\n",
        "with gzip.open(\"/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/FastText/ft_PCA_reduced-vectors.gz\", \"wt\") as f:\n",
        "    for i, word in enumerate(model.index2word):\n",
        "        vector_str = \" \".join([str(x) for x in word_vectors_50d[i]])\n",
        "        f.write(f\"{word} {vector_str}\\n\")"
      ],
      "metadata": {
        "id": "x3Irn7LP9iD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the gzip.open() function is used instead of the open() function, and the file extension is changed to .gz. The \"wt\" argument specifies that the file is opened in text mode, allowing you to use the same write() function as before. When you want to read the compressed file back into a Python script, you can use the gzip.open() function again, this time with the \"rt\" argument to open the file in text mode."
      ],
      "metadata": {
        "id": "3Di5OXBT96Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reducing Word2Vec (w2v.bin)"
      ],
      "metadata": {
        "id": "DLKfLTue0soQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v.bin'\n",
        "model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "metadata": {
        "id": "2keT9m3YHN84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the word vectors from the model\n",
        "word_vectors = model.vectors\n",
        "\n",
        "# Reduce the dimensionality of the vectors to 50 using PCA\n",
        "pca = PCA(n_components=50)\n",
        "word_vectors_50d = pca.fit_transform(word_vectors)"
      ],
      "metadata": {
        "id": "o5Wuv7XG1B4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the reduced vectors to a file in text format\n",
        "with gzip.open(\"/content/drive/MyDrive/MYDATA/Embeddings_PreTrained/word2vec/w2v_PCA_reduced-vectors.gz\", \"wt\") as f:\n",
        "    for i, word in enumerate(model.index2word):\n",
        "        vector_str = \" \".join([str(x) for x in word_vectors_50d[i]])\n",
        "        f.write(f\"{word} {vector_str}\\n\")"
      ],
      "metadata": {
        "id": "7IYM3kKQ1DTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}